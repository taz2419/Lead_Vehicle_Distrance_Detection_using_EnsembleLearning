{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-06T16:09:57.099481Z",
     "iopub.status.busy": "2025-12-06T16:09:57.098740Z",
     "iopub.status.idle": "2025-12-06T16:10:28.988716Z",
     "shell.execute_reply": "2025-12-06T16:10:28.987300Z",
     "shell.execute_reply.started": "2025-12-06T16:09:57.099441Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:11:04.967275Z",
     "iopub.status.busy": "2025-12-06T16:11:04.965798Z",
     "iopub.status.idle": "2025-12-06T16:11:11.145189Z",
     "shell.execute_reply": "2025-12-06T16:11:11.143861Z",
     "shell.execute_reply.started": "2025-12-06T16:11:04.967227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q pyquaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:11:17.384674Z",
     "iopub.status.busy": "2025-12-06T16:11:17.383829Z",
     "iopub.status.idle": "2025-12-06T16:11:23.833734Z",
     "shell.execute_reply": "2025-12-06T16:11:23.832734Z",
     "shell.execute_reply.started": "2025-12-06T16:11:17.384618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pyquaternion import Quaternion\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Device setup\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# ========== DATASET PATH SELECTION ==========\n",
    "# Choose which dataset to use: Kaggle (cloud) or Local (your machine)\n",
    "USE_LOCAL_DATASET = False  # Set to True to use local dataset, False for Kaggle\n",
    "\n",
    "if USE_LOCAL_DATASET:\n",
    "    # Local dataset paths (your machine)\n",
    "    ROOT = '/media/taz/One Touch/nuscenes/v1.0-trainval01_blobs/'\n",
    "    METAROOT = '/media/taz/One Touch/nuscenes/v1.0-trainval_meta/v1.0-trainval/'\n",
    "    print(\"üîÑ Using LOCAL dataset\")\n",
    "else:\n",
    "    # Kaggle dataset paths (cloud)\n",
    "    ROOT = '/kaggle/input/nuscences-front-sensors-only/nuscenes_lead_vehicle_distance_data'\n",
    "    METAROOT = os.path.join(ROOT, 'v1.0-trainval_meta/v1.0-trainval')\n",
    "    print(\"‚òÅÔ∏è  Using KAGGLE dataset\")\n",
    "\n",
    "print(f\"‚úÖ ROOT: {ROOT}\")\n",
    "print(f\"‚úÖ METAROOT: {METAROOT}\")\n",
    "\n",
    "# ========== BEV PARAMETERS ==========\n",
    "XRANGE = (-100.0, 100.0)  # x-axis range (forward direction) in meters\n",
    "YRANGE = (-50.0, 50.0)    # y-axis range (lateral direction) in meters\n",
    "RES = 0.5                 # Resolution: meters per voxel\n",
    "\n",
    "# Compute grid dimensions\n",
    "NX = int((XRANGE[1] - XRANGE[0]) / RES)\n",
    "NY = int((YRANGE[1] - YRANGE[0]) / RES)\n",
    "\n",
    "print(f\"BEV Grid: {NX} x {NY} ({NX*NY} voxels, {RES}m resolution)\")\n",
    "\n",
    "# ========== BEV CHANNEL DEFINITIONS ==========\n",
    "BEV_CHANNELS_LIDAR = 4    # count, avg_z, max_z, avg_intensity\n",
    "BEV_CHANNELS_RADAR = 4    # count, avg_z, avg_doppler, avg_rcs\n",
    "BEV_CHANNELS_CAM = 8      # placeholder (R, G, B, Mean, Max, Std, Occupancy, Valid)\n",
    "\n",
    "print(f\"BEV Channels: LIDAR={BEV_CHANNELS_LIDAR}, RADAR={BEV_CHANNELS_RADAR}, CAM={BEV_CHANNELS_CAM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== KAGGLE PATH DIAGNOSTICS (REMOVE AFTER FIXING) ==========\n",
    "# Run this cell on Kaggle to find where the JSON files actually are\n",
    "\n",
    "if not USE_LOCAL_DATASET:\n",
    "    print(\"üîç Exploring Kaggle dataset structure...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check what's in /kaggle/input\n",
    "    print(\"\\nüìÇ Contents of /kaggle/input:\")\n",
    "    try:\n",
    "        for item in os.listdir('/kaggle/input'):\n",
    "            print(f\"   üìÅ {item}/\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")\n",
    "    \n",
    "    # Check ROOT\n",
    "    print(f\"\\nüìÇ Contents of ROOT ({ROOT}):\")\n",
    "    print(f\"   Exists: {os.path.exists(ROOT)}\")\n",
    "    try:\n",
    "        items = os.listdir(ROOT)\n",
    "        for item in items[:20]:\n",
    "            full_path = os.path.join(ROOT, item)\n",
    "            if os.path.isdir(full_path):\n",
    "                print(f\"   üìÅ {item}/\")\n",
    "            else:\n",
    "                print(f\"   üìÑ {item}\")\n",
    "        if len(items) > 20:\n",
    "            print(f\"   ... and {len(items) - 20} more\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")\n",
    "    \n",
    "    # Try checking some common paths\n",
    "    test_paths = [\n",
    "        ROOT,\n",
    "        os.path.join(ROOT, 'v1.0-trainval_meta'),\n",
    "        os.path.join(ROOT, 'v1.0-trainval_meta/v1.0-trainval'),\n",
    "        os.path.join(ROOT, 'v1.0-trainval'),\n",
    "        METAROOT,\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüîç Testing potential metadata paths:\")\n",
    "    for path in test_paths:\n",
    "        exists = os.path.exists(path)\n",
    "        print(f\"   {'‚úÖ' if exists else '‚ùå'} {path}\")\n",
    "        if exists and os.path.isdir(path):\n",
    "            try:\n",
    "                files = os.listdir(path)\n",
    "                json_count = sum(1 for f in files if f.endswith('.json'))\n",
    "                print(f\"      ({len(files)} items, {json_count} .json files)\")\n",
    "                if json_count > 0:\n",
    "                    print(f\"      Sample files: {[f for f in files if f.endswith('.json')][:3]}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"Skipping diagnostic (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== LOAD JSON UTILITY FUNCTION ==========\n",
    "def load_json(name):\n",
    "    \"\"\"Load JSON metadata file from METAROOT\"\"\"\n",
    "    path = os.path.join(METAROOT, name)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ùå File not found: {path}\")\n",
    "        print(f\"Available files in {METAROOT}:\")\n",
    "        if os.path.exists(METAROOT):\n",
    "            for f in os.listdir(METAROOT)[:10]:\n",
    "                print(f\"  - {f}\")\n",
    "        raise FileNotFoundError(f\"Cannot find {name} in {METAROOT}\")\n",
    "    print(f'Loading: {path}')\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ========== DATASET LOADING ==========\n",
    "# Load metadata - works for both local and Kaggle datasets\n",
    "# Uses ROOT and METAROOT paths defined in cell 3 via USE_LOCAL_DATASET flag\n",
    "\n",
    "if USE_LOCAL_DATASET:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîÑ LOADING LOCAL DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìÇ Using dataset paths (defined in cell 3):\")\n",
    "    print(f\"   ROOT: {ROOT}\")\n",
    "    print(f\"   METAROOT: {METAROOT}\")\n",
    "    \n",
    "    # Check if paths exist\n",
    "    if not os.path.exists(ROOT):\n",
    "        print(f\"\\n‚ùå ERROR: ROOT does not exist!\")\n",
    "        print(f\"   Expected: {ROOT}\")\n",
    "        raise FileNotFoundError(f\"Local dataset not found at {ROOT}\")\n",
    "    \n",
    "    if not os.path.exists(METAROOT):\n",
    "        print(f\"\\n‚ùå ERROR: METAROOT does not exist!\")\n",
    "        print(f\"   Expected: {METAROOT}\")\n",
    "        raise FileNotFoundError(f\"Local metadata not found at {METAROOT}\")\n",
    "    \n",
    "    print(\"‚úÖ Paths verified\")\n",
    "    \n",
    "    print(\"\\nüì• Loading local metadata...\")\n",
    "    samples = load_json('sample.json')\n",
    "    sample_data = load_json('sample_data.json')\n",
    "    calibrated_sensor = load_json('calibrated_sensor.json')\n",
    "    ego_pose = load_json('ego_pose.json')\n",
    "    sensor = load_json('sensor.json')\n",
    "    sample_annotation = load_json('sample_annotation.json')\n",
    "    scene = load_json('scene.json')\n",
    "    \n",
    "    print('‚úÖ Loaded tables:', len(samples), 'samples,', len(sample_data), 'sample_data')\n",
    "else:\n",
    "    # Kaggle dataset - metadata will be loaded in cell 5\n",
    "    print(\"üìç Skipping local loading (will load Kaggle dataset in next cell)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:11:27.111477Z",
     "iopub.status.busy": "2025-12-06T16:11:27.110860Z",
     "iopub.status.idle": "2025-12-06T16:12:30.504347Z",
     "shell.execute_reply": "2025-12-06T16:12:30.503349Z",
     "shell.execute_reply.started": "2025-12-06T16:11:27.111440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========== KAGGLE DATASET LOADING ==========\n",
    "# Load metadata from Kaggle dataset (only runs when USE_LOCAL_DATASET = False)\n",
    "\n",
    "if not USE_LOCAL_DATASET:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚òÅÔ∏è  LOADING KAGGLE DATASET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìÇ Using dataset paths (defined in cell 3):\")\n",
    "    print(f\"   ROOT: {ROOT}\")\n",
    "    print(f\"   METAROOT: {METAROOT}\")\n",
    "    \n",
    "    # Check if paths exist\n",
    "    if not os.path.exists(ROOT):\n",
    "        print(f\"\\n‚ùå ERROR: ROOT does not exist!\")\n",
    "        print(f\"   Expected: {ROOT}\")\n",
    "        raise FileNotFoundError(f\"Kaggle dataset not found at {ROOT}\")\n",
    "    \n",
    "    if not os.path.exists(METAROOT):\n",
    "        print(f\"\\n‚ùå ERROR: METAROOT does not exist!\")\n",
    "        print(f\"   Expected: {METAROOT}\")\n",
    "        raise FileNotFoundError(f\"Kaggle metadata not found at {METAROOT}\")\n",
    "    \n",
    "    print(\"‚úÖ Paths verified\")\n",
    "    \n",
    "    print(\"\\nüì• Loading Kaggle metadata...\")\n",
    "    samples = load_json('sample.json')\n",
    "    sample_data = load_json('sample_data.json')\n",
    "    calibrated_sensor = load_json('calibrated_sensor.json')\n",
    "    ego_pose = load_json('ego_pose.json')\n",
    "    sensor = load_json('sensor.json')\n",
    "    sample_annotation = load_json('sample_annotation.json')\n",
    "    scene = load_json('scene.json')\n",
    "    \n",
    "    print('‚úÖ Loaded tables:', len(samples), 'samples,', len(sample_data), 'sample_data')\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\nüìç Skipping Kaggle loading (using local dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:12:38.899401Z",
     "iopub.status.busy": "2025-12-06T16:12:38.898993Z",
     "iopub.status.idle": "2025-12-06T16:12:45.736385Z",
     "shell.execute_reply": "2025-12-06T16:12:45.735244Z",
     "shell.execute_reply.started": "2025-12-06T16:12:38.899373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Token -> record lookup dicts\n",
    "sd_by_token = {rec['token']: rec for rec in sample_data}\n",
    "cs_by_token = {rec['token']: rec for rec in calibrated_sensor}\n",
    "ep_by_token = {rec['token']: rec for rec in ego_pose}\n",
    "sample_by_token = {rec['token']: rec for rec in samples}\n",
    "\n",
    "def sensor_channel_from_filename(fn):\n",
    "    parts = fn.split('/')\n",
    "    if len(parts) >= 3:\n",
    "        return parts[1]  # folder name\n",
    "    return parts[1]\n",
    "\n",
    "# sample_token -> {channel: sample_data_token}\n",
    "sample_to_sensor = {}\n",
    "bad_sd = 0\n",
    "for sd in sample_data:\n",
    "    samp_tok = sd.get('sample_token')\n",
    "    fn = sd.get('filename', '')\n",
    "    chan = sensor_channel_from_filename(fn)\n",
    "    if samp_tok is None or chan is None:\n",
    "        bad_sd += 1\n",
    "        continue\n",
    "    if samp_tok not in sample_to_sensor:\n",
    "        sample_to_sensor[samp_tok] = {}\n",
    "    sample_to_sensor[samp_tok][chan] = sd['token']\n",
    "    if not sd.get('is_keyframe', False):\n",
    "        continue  # Only keep keyframes for now\n",
    "\n",
    "print('Built sample_to_sensor for', len(sample_to_sensor), 'samples (skipped', bad_sd, 'sample_data records)')\n",
    "first_sample_token = samples[0]['token']\n",
    "print('Channels for first sample:', list(sample_to_sensor.get(first_sample_token, {}).keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:52:22.076611Z",
     "iopub.status.busy": "2025-12-06T16:52:22.076096Z",
     "iopub.status.idle": "2025-12-06T16:52:29.609772Z",
     "shell.execute_reply": "2025-12-06T16:52:29.608484Z",
     "shell.execute_reply.started": "2025-12-06T16:52:22.076576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========== HELPER: SENSOR PATH RESOLUTION ==========\n",
    "def abs_sensor_path(sd_rec):\n",
    "    \"\"\"Convert sample_data record to absolute file path.\"\"\"\n",
    "    filename = sd_rec.get('filename', '')\n",
    "    return os.path.join(ROOT, filename)\n",
    "\n",
    "\n",
    "# ========== LOADERS: LIDAR + RADAR ==========\n",
    "def load_lidar_points(sd_rec):\n",
    "    \"\"\"Load LIDAR points and return as (N, 4) array [x, y, z, intensity]\"\"\"\n",
    "    path = abs_sensor_path(sd_rec)\n",
    "    if not os.path.exists(path):\n",
    "        return np.zeros((0, 4), dtype=np.float32)\n",
    "\n",
    "    pts = np.fromfile(path, dtype=np.float32)\n",
    "    if pts.size == 0:\n",
    "        return np.zeros((0, 4), dtype=np.float32)\n",
    "\n",
    "    ncols = 5  # Standard nuScenes LIDAR: x,y,z,intensity,ring\n",
    "    if pts.size % ncols != 0:\n",
    "        return np.zeros((0, 4), dtype=np.float32)\n",
    "\n",
    "    pts = pts.reshape(-1, ncols)[:, :4]  # Keep only x,y,z,i\n",
    "    pts = np.clip(pts, -1000, 1000)  # Safe bounds\n",
    "    pts = np.nan_to_num(pts, nan=0.0, posinf=0.0, neginf=0.0)  # keep finite\n",
    "    return pts.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_radar_points(sd_rec):\n",
    "    \"\"\"Load RADAR points and return as (N, 5) array [x, y, z, doppler, rcs]. Robust to NaNs.\"\"\"\n",
    "    path = abs_sensor_path(sd_rec)\n",
    "    if not os.path.exists(path):\n",
    "        return np.zeros((0, 5), dtype=np.float32)\n",
    "\n",
    "    try:\n",
    "        pts = np.fromfile(path, dtype=np.float32)\n",
    "        if pts.size == 0:\n",
    "            return np.zeros((0, 5), dtype=np.float32)\n",
    "\n",
    "        # Try common RADAR formats\n",
    "        possible_cols = [18, 20, 24]\n",
    "        ncols = next((c for c in possible_cols if pts.size % c == 0), None)\n",
    "        if ncols is None:\n",
    "            return np.zeros((0, 5), dtype=np.float32)\n",
    "\n",
    "        pts = pts.reshape(-1, ncols)\n",
    "\n",
    "        # Extract fields safely with defaults\n",
    "        x = pts[:, 0] if pts.shape[1] > 0 else np.zeros(pts.shape[0])\n",
    "        y = pts[:, 1] if pts.shape[1] > 1 else np.zeros(pts.shape[0])\n",
    "        z = pts[:, 2] if pts.shape[1] > 2 else np.zeros(pts.shape[0])\n",
    "        vx = pts[:, 6] if pts.shape[1] > 6 else np.zeros(pts.shape[0])\n",
    "        vy = pts[:, 7] if pts.shape[1] > 7 else np.zeros(pts.shape[0])\n",
    "        rcs = pts[:, 8] if pts.shape[1] > 8 else np.zeros(pts.shape[0])\n",
    "\n",
    "        # Clip to valid ranges\n",
    "        x = np.clip(x, -100, 100)\n",
    "        y = np.clip(y, -100, 100)\n",
    "        z = np.clip(z, -10, 10)\n",
    "        vx = np.clip(vx, -50, 50)\n",
    "        vy = np.clip(vy, -50, 50)\n",
    "        rcs = np.clip(rcs, -50, 50)\n",
    "\n",
    "        # Compute doppler magnitude safely\n",
    "        vx = np.nan_to_num(vx, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        vy = np.nan_to_num(vy, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        vel_sq = np.clip(vx**2 + vy**2, 0, 1e3)\n",
    "        doppler = np.sqrt(vel_sq)\n",
    "\n",
    "        # Ensure finite\n",
    "        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        y = np.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        z = np.nan_to_num(z, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        doppler = np.nan_to_num(doppler, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        rcs = np.nan_to_num(rcs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        out = np.stack([x, y, z, doppler, rcs], axis=1)\n",
    "        return out.astype(np.float32)\n",
    "    except Exception:\n",
    "        return np.zeros((0, 5), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:54:48.165034Z",
     "iopub.status.busy": "2025-12-06T16:54:48.164567Z",
     "iopub.status.idle": "2025-12-06T16:54:48.449795Z",
     "shell.execute_reply": "2025-12-06T16:54:48.448757Z",
     "shell.execute_reply.started": "2025-12-06T16:54:48.165003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========== BEV BUILDERS ==========\n",
    "\n",
    "def compute_distance_to_lead_vehicle(pts_lidar):\n",
    "    \"\"\"\n",
    "    Compute distance to lead vehicle from LIDAR points.\n",
    "    Returns the minimum distance of points in front (x > 0.5m).\n",
    "    \"\"\"\n",
    "    if pts_lidar.shape[0] == 0:\n",
    "        return 50.0  # Default distance if no points\n",
    "\n",
    "    # Filter points in front (x > 0.5m ego margin)\n",
    "    front_mask = pts_lidar[:, 0] > 0.5\n",
    "    if not front_mask.any():\n",
    "        return 50.0\n",
    "\n",
    "    front_pts = pts_lidar[front_mask]\n",
    "    # Distance along x-axis (forward direction)\n",
    "    distances = front_pts[:, 0]\n",
    "    dist = np.min(distances)\n",
    "    \n",
    "    # Clamp to valid range [1, 50]\n",
    "    return np.clip(dist, 1.0, 50.0)\n",
    "\n",
    "\n",
    "def build_lidar_bev_from_points(pts_lidar, nx=NX, ny=NY, res=RES, xrange=XRANGE, yrange=YRANGE):\n",
    "    \"\"\"\n",
    "    Build LIDAR BEV occupancy grid (4 channels: count, avg_z, max_z, avg_intensity).\n",
    "    \n",
    "    Args:\n",
    "        pts_lidar: (N, 4) array [x, y, z, intensity]\n",
    "        nx, ny: BEV grid size\n",
    "        res: resolution (m/cell)\n",
    "        xrange, yrange: (min, max) ranges\n",
    "    \n",
    "    Returns:\n",
    "        (4, NX, NY) numpy array\n",
    "    \"\"\"\n",
    "    bev = np.zeros((4, nx, ny), dtype=np.float32)\n",
    "    \n",
    "    if pts_lidar.shape[0] == 0:\n",
    "        return bev\n",
    "    \n",
    "    # Quantize points to grid\n",
    "    x, y, z, intensity = pts_lidar[:, 0], pts_lidar[:, 1], pts_lidar[:, 2], pts_lidar[:, 3]\n",
    "    \n",
    "    # Map coordinates to grid indices\n",
    "    ix = ((x - xrange[0]) / res).astype(np.int32)\n",
    "    iy = ((y - yrange[0]) / res).astype(np.int32)\n",
    "    \n",
    "    # Filter in-bounds points\n",
    "    valid = (ix >= 0) & (ix < nx) & (iy >= 0) & (iy < ny)\n",
    "    ix, iy = ix[valid], iy[valid]\n",
    "    z, intensity = z[valid], intensity[valid]\n",
    "    \n",
    "    # Build occupancy grid\n",
    "    # Channel 0: point count per cell\n",
    "    np.add.at(bev[0], (ix, iy), 1)\n",
    "    \n",
    "    # Channel 1: average z per cell\n",
    "    np.add.at(bev[1], (ix, iy), z)\n",
    "    \n",
    "    # Channel 2: max z per cell\n",
    "    np.maximum.at(bev[2], (ix, iy), z)\n",
    "    \n",
    "    # Channel 3: average intensity per cell\n",
    "    np.add.at(bev[3], (ix, iy), intensity)\n",
    "    \n",
    "    # Normalize channels\n",
    "    count_nonzero = (bev[0] > 0).astype(np.float32)\n",
    "    bev[1] = np.divide(bev[1], bev[0], where=count_nonzero > 0, out=np.zeros_like(bev[1]))\n",
    "    bev[3] = np.divide(bev[3], bev[0], where=count_nonzero > 0, out=np.zeros_like(bev[3]))\n",
    "    \n",
    "    # Ensure finite\n",
    "    bev = np.nan_to_num(bev, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return bev\n",
    "\n",
    "\n",
    "def build_radar_bev_from_points(pts_radar, nx=NX, ny=NY, res=RES, xrange=XRANGE, yrange=YRANGE):\n",
    "    \"\"\"\n",
    "    Build RADAR BEV occupancy grid (4 channels: count, avg_z, avg_doppler, avg_rcs).\n",
    "    \n",
    "    Args:\n",
    "        pts_radar: (N, 5) array [x, y, z, doppler, rcs]\n",
    "        nx, ny: BEV grid size\n",
    "        res: resolution (m/cell)\n",
    "        xrange, yrange: (min, max) ranges\n",
    "    \n",
    "    Returns:\n",
    "        (4, NX, NY) numpy array\n",
    "    \"\"\"\n",
    "    bev = np.zeros((4, nx, ny), dtype=np.float32)\n",
    "    \n",
    "    if pts_radar.shape[0] == 0:\n",
    "        return bev\n",
    "    \n",
    "    # Quantize points to grid\n",
    "    x, y, z, doppler, rcs = (pts_radar[:, i] for i in range(5))\n",
    "    \n",
    "    # Map coordinates to grid indices\n",
    "    ix = ((x - xrange[0]) / res).astype(np.int32)\n",
    "    iy = ((y - yrange[0]) / res).astype(np.int32)\n",
    "    \n",
    "    # Filter in-bounds points\n",
    "    valid = (ix >= 0) & (ix < nx) & (iy >= 0) & (iy < ny)\n",
    "    ix, iy = ix[valid], iy[valid]\n",
    "    z, doppler, rcs = z[valid], doppler[valid], rcs[valid]\n",
    "    \n",
    "    # Build occupancy grid\n",
    "    # Channel 0: point count per cell\n",
    "    np.add.at(bev[0], (ix, iy), 1)\n",
    "    \n",
    "    # Channel 1: average z per cell\n",
    "    np.add.at(bev[1], (ix, iy), z)\n",
    "    \n",
    "    # Channel 2: average doppler per cell\n",
    "    np.add.at(bev[2], (ix, iy), doppler)\n",
    "    \n",
    "    # Channel 3: average rcs per cell\n",
    "    np.add.at(bev[3], (ix, iy), rcs)\n",
    "    \n",
    "    # Normalize channels\n",
    "    count_nonzero = (bev[0] > 0).astype(np.float32)\n",
    "    bev[1] = np.divide(bev[1], bev[0], where=count_nonzero > 0, out=np.zeros_like(bev[1]))\n",
    "    bev[2] = np.divide(bev[2], bev[0], where=count_nonzero > 0, out=np.zeros_like(bev[2]))\n",
    "    bev[3] = np.divide(bev[3], bev[0], where=count_nonzero > 0, out=np.zeros_like(bev[3]))\n",
    "    \n",
    "    # Ensure finite\n",
    "    bev = np.nan_to_num(bev, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return bev\n",
    "\n",
    "\n",
    "def build_camera_bev_placeholder(nx=NX, ny=NY):\n",
    "    \"\"\"\n",
    "    Build placeholder CAMERA BEV (8 channels, all zeros for now).\n",
    "    To be replaced with actual camera BEV construction from images.\n",
    "    \n",
    "    Args:\n",
    "        nx, ny: BEV grid size\n",
    "    \n",
    "    Returns:\n",
    "        (8, NX, NY) numpy array of zeros\n",
    "    \"\"\"\n",
    "    return np.zeros((BEV_CHANNELS_CAM, nx, ny), dtype=np.float32)\n",
    "\n",
    "\n",
    "print(\"‚úÖ BEV BUILDERS READY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATASET CLASS ==========\n",
    "\n",
    "class NuScenesBEVDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Multi-sensor BEV dataset for lead vehicle distance prediction.\n",
    "    Returns: (lidar_bev, camera_bev, radar_bev, distance_label)\n",
    "    \n",
    "    Distance labels are COMPUTED from LIDAR point clouds (closest front point).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sample_tokens):\n",
    "        \"\"\"Filter to only samples with existing LIDAR files\"\"\"\n",
    "        valid_tokens = []\n",
    "        for tok in sample_tokens:\n",
    "            sensors = sample_to_sensor.get(tok, {})\n",
    "            lid_tok = sensors.get(\"LIDAR_TOP\", None)\n",
    "            if lid_tok is None:\n",
    "                continue\n",
    "\n",
    "            sd_lidar = sd_by_token[lid_tok]\n",
    "            lid_path = abs_sensor_path(sd_lidar)\n",
    "            if not os.path.exists(lid_path):\n",
    "                continue\n",
    "\n",
    "            valid_tokens.append(tok)\n",
    "\n",
    "        self.sample_tokens = valid_tokens\n",
    "        print(f\"‚úÖ Dataset: {len(self.sample_tokens)} samples with LIDAR_TOP\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        token = self.sample_tokens[idx]\n",
    "        sensors = sample_to_sensor[token]\n",
    "\n",
    "        # ----- LIDAR_TOP -----\n",
    "        sd_lidar_token = sensors[\"LIDAR_TOP\"]\n",
    "        sd_lidar = sd_by_token[sd_lidar_token]\n",
    "        pts_lidar = load_lidar_points(sd_lidar)\n",
    "        lidar_bev = build_lidar_bev_from_points(pts_lidar)\n",
    "        \n",
    "        # ----- COMPUTE DISTANCE LABEL from LIDAR -----\n",
    "        distance = compute_distance_to_lead_vehicle(pts_lidar)\n",
    "\n",
    "        # ----- RADAR (fuse FRONT radars) -----\n",
    "        radar_points = []\n",
    "        for key in [\"RADAR_FRONT\", \"RADAR_FRONT_LEFT\", \"RADAR_FRONT_RIGHT\"]:\n",
    "            if key in sensors:\n",
    "                sd_tok = sensors[key]\n",
    "                sd_r = sd_by_token[sd_tok]\n",
    "                pts_r = load_radar_points(sd_r)\n",
    "                if pts_r.shape[0] > 0:\n",
    "                    radar_points.append(pts_r)\n",
    "\n",
    "        if radar_points:\n",
    "            radar_points = np.concatenate(radar_points, axis=0)\n",
    "        else:\n",
    "            radar_points = np.zeros((0, 5), dtype=np.float32)\n",
    "        radar_bev = build_radar_bev_from_points(radar_points)\n",
    "\n",
    "        # ----- CAMERA (placeholder) -----\n",
    "        cam_bev = build_camera_bev_placeholder()\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(lidar_bev),\n",
    "            torch.from_numpy(cam_bev),\n",
    "            torch.from_numpy(radar_bev),\n",
    "            torch.tensor(distance, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ DATASET CLASS READY (using computed distance labels)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:55:50.815761Z",
     "iopub.status.busy": "2025-12-06T16:55:50.815242Z",
     "iopub.status.idle": "2025-12-06T16:55:52.108850Z",
     "shell.execute_reply": "2025-12-06T16:55:52.107699Z",
     "shell.execute_reply.started": "2025-12-06T16:55:50.815730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "def visualize_bev_channels(bev_array, sensor_name=\"\", title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Visualize all channels of a BEV representation.\n",
    "    \n",
    "    Args:\n",
    "        bev_array: (C, H, W) numpy array\n",
    "        sensor_name: name of sensor (LIDAR, RADAR, etc)\n",
    "        title_prefix: prefix for subplot titles\n",
    "    \n",
    "    Returns:\n",
    "        fig, axes for further customization\n",
    "    \"\"\"\n",
    "    c, h, w = bev_array.shape\n",
    "    fig, axes = plt.subplots(1, c, figsize=(15, 3))\n",
    "    if c == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    channel_labels = {\n",
    "        'LIDAR': ['Count', 'Avg Height', 'Max Height', 'Avg Intensity'],\n",
    "        'RADAR': ['Count', 'Avg Z', 'Avg Doppler', 'Avg RCS'],\n",
    "        'CAMERA': ['R', 'G', 'B', 'Mean', 'Max', 'Std', 'Occupancy', 'Valid']\n",
    "    }\n",
    "    \n",
    "    labels = channel_labels.get(sensor_name, [f'Ch {i}' for i in range(c)])\n",
    "    cmaps = ['hot', 'cool', 'viridis', 'plasma', 'RdYlGn', 'Blues', 'Purples', 'Greys']\n",
    "    \n",
    "    for i in range(c):\n",
    "        ch_data = bev_array[i]\n",
    "        nonzero_count = (ch_data > 0).sum()\n",
    "        vmax = ch_data.max()\n",
    "        \n",
    "        im = axes[i].imshow(ch_data, cmap=cmaps[i % len(cmaps)], aspect='auto')\n",
    "        axes[i].set_title(f'{title_prefix}{labels[i] if i < len(labels) else f\"Ch{i}\"}\\n' + \n",
    "                         f'Nonzero: {nonzero_count} | Max: {vmax:.2f}')\n",
    "        axes[i].set_xlabel('Y (m)')\n",
    "        axes[i].set_ylabel('X (m)')\n",
    "        plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def visualize_bev_fusion(lidar_bev, camera_bev, radar_bev, distance_label=None):\n",
    "    \"\"\"\n",
    "    Comprehensive BEV fusion visualization showing all three modalities.\n",
    "    \n",
    "    Args:\n",
    "        lidar_bev: (4, NX, NY) LIDAR BEV\n",
    "        camera_bev: (8, NX, NY) CAMERA BEV  \n",
    "        radar_bev: (4, NX, NY) RADAR BEV\n",
    "        distance_label: scalar distance to lead vehicle\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 5, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # Row 1: LIDAR channels\n",
    "    lidar_labels = ['Count', 'Avg Height', 'Max Height', 'Avg Intensity']\n",
    "    for i in range(4):\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "        ch = lidar_bev[i]\n",
    "        nonzero = (ch > 0).sum()\n",
    "        im = ax.imshow(ch, cmap='hot', aspect='auto')\n",
    "        ax.set_title(f'LIDAR {lidar_labels[i]}\\n({nonzero} voxels, max: {ch.max():.2f})')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    # Row 1, Col 5: LIDAR summary\n",
    "    ax_summary_l = fig.add_subplot(gs[0, 4])\n",
    "    ax_summary_l.axis('off')\n",
    "    lidar_stats = f\"\"\"LIDAR Statistics:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Count voxels: {(lidar_bev[0] > 0).sum()}\n",
    "    Avg height: {lidar_bev[1][lidar_bev[0] > 0].mean():.3f}m\n",
    "    Max height: {lidar_bev[2].max():.3f}m\n",
    "    Intensity: {lidar_bev[3][lidar_bev[0] > 0].mean():.3f}\n",
    "    Total energy: {lidar_bev.sum():.1f}\"\"\"\n",
    "    ax_summary_l.text(0.05, 0.95, lidar_stats, transform=ax_summary_l.transAxes,\n",
    "                     fontfamily='monospace', fontsize=10, verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Row 2: RADAR channels\n",
    "    radar_labels = ['Count', 'Avg Z', 'Avg Doppler', 'Avg RCS']\n",
    "    for i in range(4):\n",
    "        ax = fig.add_subplot(gs[1, i])\n",
    "        ch = radar_bev[i]\n",
    "        nonzero = (ch > 0).sum()\n",
    "        im = ax.imshow(ch, cmap='plasma', aspect='auto')\n",
    "        ax.set_title(f'RADAR {radar_labels[i]}\\n({nonzero} voxels, max: {ch.max():.2f})')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    # Row 2, Col 5: RADAR summary\n",
    "    ax_summary_r = fig.add_subplot(gs[1, 4])\n",
    "    ax_summary_r.axis('off')\n",
    "    radar_stats = f\"\"\"RADAR Statistics:\n",
    "    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    Count voxels: {(radar_bev[0] > 0).sum()}\n",
    "    Avg Z: {radar_bev[1][radar_bev[0] > 0].mean():.3f}m\n",
    "    Max Doppler: {radar_bev[2].max():.3f}m/s\n",
    "    RCS: {radar_bev[3][radar_bev[0] > 0].mean():.3f}dB\n",
    "    Total energy: {radar_bev.sum():.1f}\"\"\"\n",
    "    ax_summary_r.text(0.05, 0.95, radar_stats, transform=ax_summary_r.transAxes,\n",
    "                     fontfamily='monospace', fontsize=10, verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "    \n",
    "    # Row 3: CAMERA (first 4 channels) + Distance + Fusion\n",
    "    camera_labels = ['R', 'G', 'B', 'Mean']\n",
    "    for i in range(4):\n",
    "        ax = fig.add_subplot(gs[2, i])\n",
    "        ch = camera_bev[i]\n",
    "        nonzero = (ch > 0).sum()\n",
    "        im = ax.imshow(ch, cmap='viridis', aspect='auto')\n",
    "        ax.set_title(f'CAMERA {camera_labels[i]}\\n({nonzero} voxels, max: {ch.max():.2f})')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    # Row 3, Col 5: Distance label info\n",
    "    ax_dist = fig.add_subplot(gs[2, 4])\n",
    "    ax_dist.axis('off')\n",
    "    if distance_label is not None:\n",
    "        dist_info = f\"\"\"Lead Vehicle:\n",
    "        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        Distance: {distance_label:.2f}m\n",
    "        \n",
    "        Range: [1m, 50m]\n",
    "        Status: {'‚úì Valid' if 1.0 <= distance_label <= 50.0 else '‚úó Invalid'}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        dist_info = \"\"\"Lead Vehicle:\n",
    "        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "        Distance: N/A\n",
    "        \"\"\"\n",
    "    ax_dist.text(0.05, 0.95, dist_info, transform=ax_dist.transAxes,\n",
    "                fontfamily='monospace', fontsize=10, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('Multi-Sensor BEV Fusion Visualization', fontsize=16, fontweight='bold', y=0.995)\n",
    "    return fig\n",
    "\n",
    "def verify_and_visualize_bev(dataset, name=\"dataset\", max_samples=3):\n",
    "    \"\"\"Verify BEV data integrity and visualize samples with comprehensive analysis\"\"\"\n",
    "    print(f\"\\nüé® BEV VERIFICATION: {name} ({len(dataset)} samples)\")\n",
    "    \n",
    "    if len(dataset) == 0:\n",
    "        print(\"‚ùå Dataset is empty!\")\n",
    "        return False\n",
    "    \n",
    "    lidar_ok, radar_ok, fusion_ok = 0, 0, 0\n",
    "    distances = []\n",
    "    all_lidar_voxels = []\n",
    "    all_radar_voxels = []\n",
    "    \n",
    "    for i in range(min(max_samples, len(dataset))):\n",
    "        try:\n",
    "            lidar_bev, cam_bev, radar_bev, distance = dataset[i]\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if hasattr(lidar_bev, 'numpy'):\n",
    "                lidar_bev = lidar_bev.numpy()\n",
    "            if hasattr(cam_bev, 'numpy'):\n",
    "                cam_bev = cam_bev.numpy()\n",
    "            if hasattr(radar_bev, 'numpy'):\n",
    "                radar_bev = radar_bev.numpy()\n",
    "            if hasattr(distance, 'item'):\n",
    "                distance = distance.item()\n",
    "            \n",
    "            # Count non-zero voxels\n",
    "            lidar_vox = (lidar_bev > 0).sum()\n",
    "            radar_vox = (radar_bev > 0).sum()\n",
    "            distances.append(distance)\n",
    "            all_lidar_voxels.append(lidar_vox)\n",
    "            all_radar_voxels.append(radar_vox)\n",
    "            \n",
    "            if lidar_vox > 0:\n",
    "                lidar_ok += 1\n",
    "            if radar_vox > 0:\n",
    "                radar_ok += 1\n",
    "            if (lidar_vox > 0) and (radar_vox > 0):\n",
    "                fusion_ok += 1\n",
    "            \n",
    "            # Create detailed fusion visualization\n",
    "            print(f\"\\n  Sample {i+1}/{min(max_samples, len(dataset))}:\")\n",
    "            print(f\"    LIDAR:    {lidar_vox} voxels | Height: [{lidar_bev[1].min():.2f}, {lidar_bev[1].max():.2f}]m\")\n",
    "            print(f\"    RADAR:    {radar_vox} voxels | Doppler: [{radar_bev[2].min():.2f}, {radar_bev[2].max():.2f}]m/s\")\n",
    "            print(f\"    Distance: {distance:.2f}m ‚úì\")\n",
    "            \n",
    "            # Show fusion visualization\n",
    "            fig = visualize_bev_fusion(lidar_bev, cam_bev, radar_bev, distance)\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error loading sample {i}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä {name.upper()} VERIFICATION SUMMARY:\")\n",
    "    print(f\"  ‚úÖ Samples processed: {min(max_samples, len(dataset))}\")\n",
    "    print(f\"  ‚úÖ LIDAR valid:      {lidar_ok}/{min(max_samples, len(dataset))}\")\n",
    "    print(f\"  ‚úÖ RADAR valid:      {radar_ok}/{min(max_samples, len(dataset))}\")\n",
    "    print(f\"  ‚úÖ Fusion ready:     {fusion_ok}/{min(max_samples, len(dataset))}\")\n",
    "    \n",
    "    if len(distances) > 0:\n",
    "        print(f\"\\n  üìè Distance Statistics:\")\n",
    "        print(f\"    Range:    {min(distances):.2f}m - {max(distances):.2f}m\")\n",
    "        print(f\"    Mean:     {np.mean(distances):.2f}m\")\n",
    "        print(f\"    Median:   {np.median(distances):.2f}m\")\n",
    "        print(f\"    Std:      {np.std(distances):.2f}m\")\n",
    "    \n",
    "    if len(all_lidar_voxels) > 0:\n",
    "        print(f\"\\n  üü° LIDAR Point Density:\")\n",
    "        print(f\"    Mean voxels: {np.mean(all_lidar_voxels):.0f}\")\n",
    "        print(f\"    Min voxels:  {np.min(all_lidar_voxels):.0f}\")\n",
    "        print(f\"    Max voxels:  {np.max(all_lidar_voxels):.0f}\")\n",
    "    \n",
    "    if len(all_radar_voxels) > 0:\n",
    "        print(f\"\\n  üî¥ RADAR Detection Density:\")\n",
    "        print(f\"    Mean voxels: {np.mean(all_radar_voxels):.0f}\")\n",
    "        print(f\"    Min voxels:  {np.min(all_radar_voxels):.0f}\")\n",
    "        print(f\"    Max voxels:  {np.max(all_radar_voxels):.0f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    \n",
    "    return lidar_ok > 0\n",
    "\n",
    "print(\"‚úÖ ENHANCED BEV VISUALIZATION FUNCTIONS READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:21:36.449163Z",
     "iopub.status.busy": "2025-12-06T16:21:36.448065Z",
     "iopub.status.idle": "2025-12-06T16:21:37.204293Z",
     "shell.execute_reply": "2025-12-06T16:21:37.202867Z",
     "shell.execute_reply.started": "2025-12-06T16:21:36.449123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========== HYPERPARAMETERS ==========\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "DBEV = 64  # Feature dimension in model\n",
    "\n",
    "print(f\"Hyperparameters: LR={LR}, EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, DBEV={DBEV}\")\n",
    "\n",
    "# ========== CREATE DATASETS + DATALOADERS ==========\n",
    "# Build 70/15/15 train/val/test split\n",
    "import random\n",
    "all_tokens = [s['token'] for s in samples]\n",
    "\n",
    "# IMPORTANT: Shuffle to distribute LIDAR samples across splits\n",
    "# (local dataset may only have partial data)\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(all_tokens)\n",
    "\n",
    "ntotal = len(all_tokens)\n",
    "ntrain = int(0.7 * ntotal)\n",
    "nval = int(0.15 * ntotal)\n",
    "\n",
    "train_tokens = all_tokens[:ntrain]\n",
    "val_tokens = all_tokens[ntrain:ntrain+nval]\n",
    "test_tokens = all_tokens[ntrain+nval:]\n",
    "\n",
    "print(f\"\\nüìä Split: {len(train_tokens)} train / {len(val_tokens)} val / {len(test_tokens)} test\")\n",
    "\n",
    "# Create datasets\n",
    "train_ds = NuScenesBEVDataset(train_tokens)\n",
    "val_ds = NuScenesBEVDataset(val_tokens)\n",
    "test_ds = NuScenesBEVDataset(test_tokens)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test one batch\n",
    "print(\"\\nüîç Testing batch loading...\")\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"  Batch size: {len(batch)} tensors\")\n",
    "print(f\"  LIDAR BEV shape: {batch[0].shape}\")\n",
    "print(f\"  CAMERA BEV shape: {batch[1].shape}\")\n",
    "print(f\"  RADAR BEV shape: {batch[2].shape}\")\n",
    "print(f\"  Distance shape: {batch[3].shape}\")\n",
    "print(\"‚úÖ Dataloaders ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== LOADING VALIDATION ==========\n",
    "# Comprehensive error checking for all loading functions\n",
    "\n",
    "print(\"üîç VALIDATING LOADING FUNCTIONS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def validate_loading_pipeline():\n",
    "    \"\"\"Test all loading functions on first few samples\"\"\"\n",
    "    \n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Test 1: abs_sensor_path for different sensor types\n",
    "    print(\"\\n[1/4] Testing sensor path resolution...\")\n",
    "    test_sample_idx = 0\n",
    "    test_token = train_tokens[test_sample_idx] if len(train_tokens) > 0 else None\n",
    "    \n",
    "    if test_token and test_token in sample_to_sensor:\n",
    "        sensors = sample_to_sensor[test_token]\n",
    "        for sensor_key in ['LIDAR_TOP', 'RADAR_FRONT']:\n",
    "            if sensor_key in sensors:\n",
    "                try:\n",
    "                    sd_rec = sd_by_token[sensors[sensor_key]]\n",
    "                    path = abs_sensor_path(sd_rec)\n",
    "                    if os.path.exists(path):\n",
    "                        print(f\"  ‚úÖ {sensor_key}: {path[:50]}...\")\n",
    "                    else:\n",
    "                        warnings.append(f\"{sensor_key} path doesn't exist: {path}\")\n",
    "                        print(f\"  ‚ö†Ô∏è  {sensor_key}: File not found\")\n",
    "                except Exception as e:\n",
    "                    errors.append(f\"Error resolving {sensor_key}: {str(e)}\")\n",
    "                    print(f\"  ‚ùå {sensor_key}: {e}\")\n",
    "    \n",
    "    # Test 2: LIDAR loading\n",
    "    print(\"\\n[2/4] Testing LIDAR loading...\")\n",
    "    if test_token and 'LIDAR_TOP' in sample_to_sensor.get(test_token, {}):\n",
    "        try:\n",
    "            sd_rec = sd_by_token[sample_to_sensor[test_token]['LIDAR_TOP']]\n",
    "            pts = load_lidar_points(sd_rec)\n",
    "            print(f\"  ‚úÖ LIDAR: {pts.shape[0]} points, shape: {pts.shape}\")\n",
    "            if pts.shape[0] == 0:\n",
    "                warnings.append(\"LIDAR returned empty point cloud\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"LIDAR loading failed: {str(e)}\")\n",
    "            print(f\"  ‚ùå LIDAR loading: {e}\")\n",
    "    \n",
    "    # Test 3: RADAR loading (multiple sensors)\n",
    "    print(\"\\n[3/4] Testing RADAR loading...\")\n",
    "    if test_token:\n",
    "        for radar_key in ['RADAR_FRONT', 'RADAR_FRONT_LEFT', 'RADAR_FRONT_RIGHT']:\n",
    "            if radar_key in sample_to_sensor.get(test_token, {}):\n",
    "                try:\n",
    "                    sd_rec = sd_by_token[sample_to_sensor[test_token][radar_key]]\n",
    "                    pts = load_radar_points(sd_rec)\n",
    "                    print(f\"  ‚úÖ {radar_key}: {pts.shape[0]} points, shape: {pts.shape}\")\n",
    "                    if pts.shape[0] == 0:\n",
    "                        warnings.append(f\"{radar_key} returned empty point cloud\")\n",
    "                except Exception as e:\n",
    "                    errors.append(f\"{radar_key} loading failed: {str(e)}\")\n",
    "                    print(f\"  ‚ùå {radar_key}: {e}\")\n",
    "    \n",
    "    # Test 4: BEV building and fusion\n",
    "    print(\"\\n[4/4] Testing BEV building and fusion...\")\n",
    "    if test_token:\n",
    "        try:\n",
    "            # Load all sensors\n",
    "            lidar_bev = np.zeros((BEV_CHANNELS_LIDAR, NX, NY), dtype=np.float32)\n",
    "            radar_bev = np.zeros((BEV_CHANNELS_RADAR, NX, NY), dtype=np.float32)\n",
    "            camera_bev = np.zeros((BEV_CHANNELS_CAM, NX, NY), dtype=np.float32)\n",
    "            \n",
    "            sensors = sample_to_sensor.get(test_token, {})\n",
    "            \n",
    "            if 'LIDAR_TOP' in sensors:\n",
    "                pts_l = load_lidar_points(sd_by_token[sensors['LIDAR_TOP']])\n",
    "                lidar_bev = build_lidar_bev_from_points(pts_l)\n",
    "            \n",
    "            radar_pts_list = []\n",
    "            for radar_key in ['RADAR_FRONT', 'RADAR_FRONT_LEFT', 'RADAR_FRONT_RIGHT']:\n",
    "                if radar_key in sensors:\n",
    "                    pts_r = load_radar_points(sd_by_token[sensors[radar_key]])\n",
    "                    if pts_r.shape[0] > 0:\n",
    "                        radar_pts_list.append(pts_r)\n",
    "            \n",
    "            if radar_pts_list:\n",
    "                radar_pts = np.concatenate(radar_pts_list, axis=0)\n",
    "                radar_bev = build_radar_bev_from_points(radar_pts)\n",
    "            \n",
    "            # Test fusion (concatenation)\n",
    "            fused = np.concatenate([lidar_bev, camera_bev, radar_bev], axis=0)\n",
    "            print(f\"  ‚úÖ LIDAR BEV:  {lidar_bev.shape}\")\n",
    "            print(f\"  ‚úÖ CAMERA BEV: {camera_bev.shape}\")\n",
    "            print(f\"  ‚úÖ RADAR BEV:  {radar_bev.shape}\")\n",
    "            print(f\"  ‚úÖ Fused:      {fused.shape} ({fused.shape[0]} channels)\")\n",
    "            \n",
    "            # Test with torch tensors\n",
    "            t_l = torch.from_numpy(lidar_bev).unsqueeze(0)\n",
    "            t_c = torch.from_numpy(camera_bev).unsqueeze(0)\n",
    "            t_r = torch.from_numpy(radar_bev).unsqueeze(0)\n",
    "            t_fused = torch.cat([t_l, t_c, t_r], dim=1)\n",
    "            print(f\"  ‚úÖ PyTorch fused: {t_fused.shape}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append(f\"BEV building/fusion failed: {str(e)}\")\n",
    "            print(f\"  ‚ùå BEV fusion: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    if errors:\n",
    "        print(f\"‚ùå ERRORS ({len(errors)}):\")\n",
    "        for err in errors:\n",
    "            print(f\"   ‚Ä¢ {err}\")\n",
    "    else:\n",
    "        print(\"‚úÖ NO CRITICAL ERRORS DETECTED\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNINGS ({len(warnings)}):\")\n",
    "        for warn in warnings:\n",
    "            print(f\"   ‚Ä¢ {warn}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    return len(errors) == 0\n",
    "\n",
    "# Run validation\n",
    "pipeline_ok = validate_loading_pipeline()\n",
    "\n",
    "if not pipeline_ok:\n",
    "    print(\"\\n‚ùå Pipeline validation failed. Check errors above before proceeding.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Pipeline validation PASSED. Safe to proceed with training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== OPTIONAL: VERIFY PIPELINE BEFORE TRAINING ==========\n",
    "# Run this cell to verify everything works correctly\n",
    "\n",
    "print(\"üîç VERIFYING PIPELINE...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify train dataset\n",
    "print(\"\\nüìã TRAIN Dataset:\")\n",
    "verify_and_visualize_bev(train_ds, \"TRAIN\", max_samples=3)\n",
    "\n",
    "# Verify val dataset\n",
    "if len(val_ds) > 0:\n",
    "    print(\"\\nüìã VAL Dataset:\")\n",
    "    verify_and_visualize_bev(val_ds, \"VAL\", max_samples=2)\n",
    "\n",
    "# Verify test dataset\n",
    "if len(test_ds) > 0:\n",
    "    print(\"\\nüìã TEST Dataset:\")\n",
    "    verify_and_visualize_bev(test_ds, \"TEST\", max_samples=2)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ PIPELINE VERIFICATION COMPLETE - Ready to train!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BEV TRANSFORMATION VISUALIZATION ==========\n",
    "# Visualize how raw sensor data transforms into Bird's Eye View\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_bev_transformation(sample_idx=0):\n",
    "    \"\"\"\n",
    "    Comprehensive visualization showing:\n",
    "    1. Raw sensor data (camera image, LIDAR point cloud)\n",
    "    2. Intermediate transformation steps\n",
    "    3. Final BEV representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a sample\n",
    "    sample_token = train_tokens[sample_idx]\n",
    "    sample_rec = sample_by_token[sample_token]\n",
    "    sensors = sample_to_sensor[sample_token]\n",
    "    \n",
    "    # Check what sensors are available\n",
    "    has_lidar = 'LIDAR_TOP' in sensors\n",
    "    has_cam_front = 'CAM_FRONT' in sensors\n",
    "    has_radar = 'RADAR_FRONT' in sensors\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # ========== ROW 1: RAW SENSOR DATA ==========\n",
    "    \n",
    "    # 1.1 Camera Image (if available)\n",
    "    ax_cam = fig.add_subplot(gs[0, :2])\n",
    "    if has_cam_front:\n",
    "        try:\n",
    "            cam_token = sensors['CAM_FRONT']\n",
    "            cam_rec = sd_by_token[cam_token]\n",
    "            cam_path = os.path.join(ROOT, cam_rec['filename'])\n",
    "            \n",
    "            if os.path.exists(cam_path):\n",
    "                img = Image.open(cam_path)\n",
    "                ax_cam.imshow(img)\n",
    "                ax_cam.set_title(f'CAM_FRONT Raw Image\\n{img.size[0]}√ó{img.size[1]} pixels', \n",
    "                               fontsize=12, fontweight='bold')\n",
    "                ax_cam.axis('off')\n",
    "            else:\n",
    "                ax_cam.text(0.5, 0.5, f'Image not found:\\n{cam_path}', \n",
    "                          ha='center', va='center', fontsize=10)\n",
    "                ax_cam.set_title('CAM_FRONT (not found)')\n",
    "                ax_cam.axis('off')\n",
    "        except Exception as e:\n",
    "            ax_cam.text(0.5, 0.5, f'Error loading camera:\\n{str(e)}', \n",
    "                      ha='center', va='center', fontsize=10)\n",
    "            ax_cam.set_title('CAM_FRONT (error)')\n",
    "            ax_cam.axis('off')\n",
    "    else:\n",
    "        ax_cam.text(0.5, 0.5, 'CAM_FRONT not available', ha='center', va='center')\n",
    "        ax_cam.set_title('CAM_FRONT')\n",
    "        ax_cam.axis('off')\n",
    "    \n",
    "    # 1.2 LIDAR Point Cloud (top-down view)\n",
    "    ax_lidar_raw = fig.add_subplot(gs[0, 2:])\n",
    "    if has_lidar:\n",
    "        lidar_token = sensors['LIDAR_TOP']\n",
    "        lidar_rec = sd_by_token[lidar_token]\n",
    "        pts = load_lidar_points(lidar_rec)\n",
    "        \n",
    "        if pts.shape[0] > 0:\n",
    "            # Plot points in ego frame (x forward, y left)\n",
    "            x, y, z, intensity = pts[:, 0], pts[:, 1], pts[:, 2], pts[:, 3]\n",
    "            \n",
    "            # Filter to BEV range\n",
    "            mask = (x >= XRANGE[0]) & (x < XRANGE[1]) & (y >= YRANGE[0]) & (y < YRANGE[1])\n",
    "            x_filt, y_filt, z_filt, i_filt = x[mask], y[mask], z[mask], intensity[mask]\n",
    "            \n",
    "            scatter = ax_lidar_raw.scatter(y_filt, x_filt, c=z_filt, s=0.5, \n",
    "                                          cmap='jet', vmin=-3, vmax=3)\n",
    "            ax_lidar_raw.set_xlim(YRANGE)\n",
    "            ax_lidar_raw.set_ylim(XRANGE)\n",
    "            ax_lidar_raw.set_xlabel('Y (m) - Left/Right')\n",
    "            ax_lidar_raw.set_ylabel('X (m) - Forward')\n",
    "            ax_lidar_raw.set_title(f'LIDAR Point Cloud (Top View)\\n{pts.shape[0]} points, colored by height', \n",
    "                                  fontsize=12, fontweight='bold')\n",
    "            ax_lidar_raw.grid(True, alpha=0.3)\n",
    "            plt.colorbar(scatter, ax=ax_lidar_raw, label='Height (m)')\n",
    "            \n",
    "            # Add ego vehicle marker\n",
    "            ego = patches.Rectangle((-1, -1.5), 2, 3, linewidth=2, \n",
    "                                   edgecolor='red', facecolor='none', label='Ego Vehicle')\n",
    "            ax_lidar_raw.add_patch(ego)\n",
    "            ax_lidar_raw.legend(loc='upper right')\n",
    "        else:\n",
    "            ax_lidar_raw.text(0.5, 0.5, 'No LIDAR points', ha='center', va='center')\n",
    "    else:\n",
    "        ax_lidar_raw.text(0.5, 0.5, 'LIDAR_TOP not available', ha='center', va='center')\n",
    "    ax_lidar_raw.set_aspect('equal')\n",
    "    \n",
    "    # ========== ROW 2: TRANSFORMATION STEPS ==========\n",
    "    \n",
    "    # 2.1 Camera Calibration Info\n",
    "    ax_calib = fig.add_subplot(gs[1, 0])\n",
    "    ax_calib.axis('off')\n",
    "    if has_cam_front:\n",
    "        try:\n",
    "            cam_token = sensors['CAM_FRONT']\n",
    "            cam_rec = sd_by_token[cam_token]\n",
    "            cs_rec = cs_by_token[cam_rec['calibrated_sensor_token']]\n",
    "            \n",
    "            # Extract calibration data\n",
    "            translation = cs_rec.get('translation', [0, 0, 0])\n",
    "            rotation = cs_rec.get('rotation', [1, 0, 0, 0])\n",
    "            camera_intrinsic = cs_rec.get('camera_intrinsic', [[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "            \n",
    "            calib_text = f\"\"\"Camera Calibration\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Translation (m):\n",
    "  X: {translation[0]:.2f}\n",
    "  Y: {translation[1]:.2f}\n",
    "  Z: {translation[2]:.2f}\n",
    "\n",
    "Rotation (quaternion):\n",
    "  [{rotation[0]:.3f}, {rotation[1]:.3f},\n",
    "   {rotation[2]:.3f}, {rotation[3]:.3f}]\n",
    "\n",
    "Intrinsics:\n",
    "  fx: {camera_intrinsic[0][0]:.1f}\n",
    "  fy: {camera_intrinsic[1][1]:.1f}\n",
    "  cx: {camera_intrinsic[0][2]:.1f}\n",
    "  cy: {camera_intrinsic[1][2]:.1f}\n",
    "\"\"\"\n",
    "            ax_calib.text(0.05, 0.95, calib_text, transform=ax_calib.transAxes,\n",
    "                        fontfamily='monospace', fontsize=9, verticalalignment='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "        except Exception as e:\n",
    "            ax_calib.text(0.5, 0.5, f'Calibration error:\\n{str(e)[:50]}', \n",
    "                        ha='center', va='center', fontsize=9)\n",
    "    else:\n",
    "        ax_calib.text(0.5, 0.5, 'No camera calibration', ha='center', va='center')\n",
    "    \n",
    "    # 2.2 Grid Discretization Info\n",
    "    ax_grid = fig.add_subplot(gs[1, 1])\n",
    "    ax_grid.axis('off')\n",
    "    grid_text = f\"\"\"BEV Grid Configuration\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "Resolution: {RES}m per pixel\n",
    "\n",
    "X Range: {XRANGE[0]:.0f}m to {XRANGE[1]:.0f}m\n",
    "Y Range: {YRANGE[0]:.0f}m to {YRANGE[1]:.0f}m\n",
    "\n",
    "Grid Size: {NX} √ó {NY}\n",
    "Total Voxels: {NX * NY:,}\n",
    "\n",
    "Channels:\n",
    "  LIDAR: {BEV_CHANNELS_LIDAR} (count, avg_h, max_h, intensity)\n",
    "  RADAR: {BEV_CHANNELS_RADAR} (count, z, doppler, rcs)\n",
    "  CAMERA: {BEV_CHANNELS_CAM} (R,G,B,mean,max,std,occ,valid)\n",
    "\n",
    "Total BEV Channels: {BEV_CHANNELS_LIDAR + BEV_CHANNELS_RADAR + BEV_CHANNELS_CAM}\n",
    "\"\"\"\n",
    "    ax_grid.text(0.05, 0.95, grid_text, transform=ax_grid.transAxes,\n",
    "                fontfamily='monospace', fontsize=9, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "    \n",
    "    # 2.3 Transformation Diagram\n",
    "    ax_transform = fig.add_subplot(gs[1, 2:])\n",
    "    ax_transform.axis('off')\n",
    "    transform_text = \"\"\"\n",
    "    3D SENSOR DATA ‚Üí BIRD'S EYE VIEW TRANSFORMATION\n",
    "    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    \n",
    "    STEP 1: Load Sensor Data\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  LIDAR Points   ‚îÇ   ‚îÇ  Camera Image   ‚îÇ   ‚îÇ  RADAR Returns  ‚îÇ\n",
    "    ‚îÇ  (x, y, z, i)   ‚îÇ   ‚îÇ  (u, v, RGB)    ‚îÇ   ‚îÇ  (x, y, v, rcs) ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ                     ‚îÇ                     ‚îÇ\n",
    "    \n",
    "    STEP 2: Transform to Ego Frame\n",
    "             ‚îÇ                     ‚îÇ                     ‚îÇ\n",
    "             ‚îÇ        Apply Camera ‚îÇ        Already in   ‚îÇ\n",
    "             ‚îÇ        Calibration  ‚îÇ        Ego Frame    ‚îÇ\n",
    "             ‚ñº                     ‚ñº                     ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ           All Sensors in Ego Vehicle Frame                  ‚îÇ\n",
    "    ‚îÇ         (Origin at ego center, X forward, Y left)           ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                ‚îÇ\n",
    "    \n",
    "    STEP 3: Project to BEV Grid\n",
    "                                ‚îÇ\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Discretize into Grid ‚îÇ\n",
    "                    ‚îÇ  (NX √ó NY voxels)     ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                ‚îÇ\n",
    "    \n",
    "    STEP 4: Aggregate Features per Voxel\n",
    "                                ‚îÇ\n",
    "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            ‚ñº                   ‚ñº                   ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ  LIDAR BEV   ‚îÇ   ‚îÇ  CAMERA BEV  ‚îÇ   ‚îÇ  RADAR BEV   ‚îÇ\n",
    "    ‚îÇ  (4, 400, 200)‚îÇ   ‚îÇ (8, 400, 200)‚îÇ   ‚îÇ (4, 400, 200)‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ                   ‚îÇ                   ‚îÇ\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                ‚îÇ\n",
    "                                ‚ñº\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ   Fused Multi-Modal   ‚îÇ\n",
    "                    ‚îÇ    BEV Representation ‚îÇ\n",
    "                    ‚îÇ    (16, 400, 200)     ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    \"\"\"\n",
    "    ax_transform.text(0.05, 0.95, transform_text, transform=ax_transform.transAxes,\n",
    "                     fontfamily='monospace', fontsize=8, verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # ========== ROW 3: FINAL BEV REPRESENTATIONS ==========\n",
    "    \n",
    "    # 3.1 LIDAR BEV\n",
    "    if has_lidar:\n",
    "        pts = load_lidar_points(lidar_rec)\n",
    "        lidar_bev = build_lidar_bev_from_points(pts)\n",
    "        \n",
    "        ax_lidar_bev = fig.add_subplot(gs[2, 0])\n",
    "        im = ax_lidar_bev.imshow(lidar_bev[0], cmap='hot', aspect='auto')\n",
    "        ax_lidar_bev.set_title(f'LIDAR BEV (Count)\\n{(lidar_bev[0] > 0).sum()} voxels', \n",
    "                              fontsize=10, fontweight='bold')\n",
    "        ax_lidar_bev.set_xlabel('Y (grid)')\n",
    "        ax_lidar_bev.set_ylabel('X (grid)')\n",
    "        plt.colorbar(im, ax=ax_lidar_bev, fraction=0.046)\n",
    "    \n",
    "    # 3.2 RADAR BEV\n",
    "    if has_radar:\n",
    "        radar_pts = []\n",
    "        for radar_key in ['RADAR_FRONT', 'RADAR_FRONT_LEFT', 'RADAR_FRONT_RIGHT']:\n",
    "            if radar_key in sensors:\n",
    "                pts_r = load_radar_points(sd_by_token[sensors[radar_key]])\n",
    "                if pts_r.shape[0] > 0:\n",
    "                    radar_pts.append(pts_r)\n",
    "        \n",
    "        if radar_pts:\n",
    "            radar_pts = np.concatenate(radar_pts, axis=0)\n",
    "            radar_bev = build_radar_bev_from_points(radar_pts)\n",
    "            \n",
    "            ax_radar_bev = fig.add_subplot(gs[2, 1])\n",
    "            im = ax_radar_bev.imshow(radar_bev[0], cmap='plasma', aspect='auto')\n",
    "            ax_radar_bev.set_title(f'RADAR BEV (Count)\\n{(radar_bev[0] > 0).sum()} voxels', \n",
    "                                  fontsize=10, fontweight='bold')\n",
    "            ax_radar_bev.set_xlabel('Y (grid)')\n",
    "            ax_radar_bev.set_ylabel('X (grid)')\n",
    "            plt.colorbar(im, ax=ax_radar_bev, fraction=0.046)\n",
    "    \n",
    "    # 3.3 Camera BEV (placeholder - shows it's empty)\n",
    "    cam_bev = build_camera_bev_placeholder()\n",
    "    ax_cam_bev = fig.add_subplot(gs[2, 2])\n",
    "    im = ax_cam_bev.imshow(cam_bev[0], cmap='viridis', aspect='auto')\n",
    "    ax_cam_bev.set_title('CAMERA BEV (Placeholder)\\nNot implemented', \n",
    "                        fontsize=10, fontweight='bold', color='orange')\n",
    "    ax_cam_bev.set_xlabel('Y (grid)')\n",
    "    ax_cam_bev.set_ylabel('X (grid)')\n",
    "    plt.colorbar(im, ax=ax_cam_bev, fraction=0.046)\n",
    "    \n",
    "    # 3.4 Implementation Status\n",
    "    ax_status = fig.add_subplot(gs[2, 3])\n",
    "    ax_status.axis('off')\n",
    "    status_text = f\"\"\"BEV Status\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "‚úÖ LIDAR BEV\n",
    "   Fully implemented\n",
    "   \n",
    "‚ö†Ô∏è  RADAR BEV\n",
    "   Working (sparse)\n",
    "   \n",
    "‚ùå CAMERA BEV\n",
    "   Placeholder only\n",
    "   \n",
    "To Implement:\n",
    "1. Load camera image\n",
    "2. Get depth estimation\n",
    "3. Project pixels to 3D\n",
    "4. Map to BEV grid\n",
    "5. Aggregate RGB values\n",
    "\"\"\"\n",
    "    ax_status.text(0.05, 0.95, status_text, transform=ax_status.transAxes,\n",
    "                  fontfamily='monospace', fontsize=9, verticalalignment='top',\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle(f'BEV Transformation Pipeline Visualization - Sample {sample_idx}', \n",
    "                fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"‚úÖ BEV transformation visualization function ready!\")\n",
    "print(\"Run: visualize_bev_transformation(sample_idx=0) to see the full pipeline\")\n",
    "\n",
    "fig = visualize_bev_transformation(sample_idx=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:21:59.590194Z",
     "iopub.status.busy": "2025-12-06T16:21:59.589708Z",
     "iopub.status.idle": "2025-12-06T16:21:59.632846Z",
     "shell.execute_reply": "2025-12-06T16:21:59.631607Z",
     "shell.execute_reply.started": "2025-12-06T16:21:59.590162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UTENet4BranchBEV(nn.Module):\n",
    "    \"\"\"\n",
    "    Uncertainty-aware 4-Branch BEV Network with Ensemble Prediction:\n",
    "    - LIDAR branch\n",
    "    - CAMERA branch  \n",
    "    - RADAR branch\n",
    "    - Fused (multi-modal) branch\n",
    "    \n",
    "    ENSEMBLE prediction: weighted average of all 4 branches.\n",
    "    Weights learned via uncertainty (log-variance) and can be modulated by weather conditions.\n",
    "    \n",
    "    Design enables: weather-adaptive weighting at inference time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, c_lidar=BEV_CHANNELS_LIDAR, c_cam=BEV_CHANNELS_CAM, \n",
    "                 c_radar=BEV_CHANNELS_RADAR, d_bev=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        def make_encoder(cin):\n",
    "            \"\"\"Conv encoder: input channels -> d_bev features\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(cin, d_bev, 3, padding=1),\n",
    "                nn.BatchNorm2d(d_bev),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(dropout),\n",
    "                nn.Conv2d(d_bev, d_bev, 3, padding=1),\n",
    "                nn.BatchNorm2d(d_bev),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(dropout)\n",
    "            )\n",
    "        \n",
    "        # Encoders for each modality\n",
    "        self.enc_lidar = make_encoder(c_lidar)\n",
    "        self.enc_cam = make_encoder(c_cam)\n",
    "        self.enc_radar = make_encoder(c_radar)\n",
    "        \n",
    "        # Fusion encoder (concatenates all 3 branches)\n",
    "        self.fuse_bev = nn.Sequential(\n",
    "            nn.Conv2d(d_bev*3, d_bev*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(d_bev*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(d_bev*2, d_bev, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(d_bev),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Prediction heads (one per branch)\n",
    "        self.head_lidar = nn.Linear(d_bev, 1)\n",
    "        self.head_cam = nn.Linear(d_bev, 1)\n",
    "        self.head_radar = nn.Linear(d_bev, 1)\n",
    "        self.head_fused = nn.Linear(d_bev, 1)\n",
    "        \n",
    "        # Learned uncertainty (log-variance) for each branch\n",
    "        # These determine the inverse-variance weights during training\n",
    "        # Can be modulated at inference time based on weather conditions\n",
    "        self.logvar_lidar = nn.Parameter(torch.zeros(()))\n",
    "        self.logvar_cam = nn.Parameter(torch.zeros(()))\n",
    "        self.logvar_radar = nn.Parameter(torch.zeros(()))\n",
    "        self.logvar_fused = nn.Parameter(torch.zeros(()))\n",
    "    \n",
    "    def forward(self, bev_lidar, bev_cam, bev_radar):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bev_lidar: (B, 4, NX, NY)\n",
    "            bev_cam: (B, 8, NX, NY)\n",
    "            bev_radar: (B, 4, NX, NY)\n",
    "        \n",
    "        Returns:\n",
    "            pred_ensemble: (B,) - weighted ensemble of all 4 branches\n",
    "            (pred_lidar, pred_cam, pred_radar, pred_fused): individual branch predictions\n",
    "            (w_lidar, w_cam, w_radar, w_fused): normalized weights (detached for inference)\n",
    "        \"\"\"\n",
    "        # Encode each modality\n",
    "        l = self.enc_lidar(bev_lidar)    # (B, d_bev, NX, NY)\n",
    "        c = self.enc_cam(bev_cam)        # (B, d_bev, NX, NY)\n",
    "        r = self.enc_radar(bev_radar)    # (B, d_bev, NX, NY)\n",
    "        \n",
    "        # Fuse all modalities\n",
    "        fused = self.fuse_bev(torch.cat([l, c, r], dim=1))  # (B, d_bev, NX, NY)\n",
    "        \n",
    "        # Pool to vectors\n",
    "        l_vec = self.pool(l).flatten(1)      # (B, d_bev)\n",
    "        c_vec = self.pool(c).flatten(1)\n",
    "        r_vec = self.pool(r).flatten(1)\n",
    "        f_vec = self.pool(fused).flatten(1)\n",
    "        \n",
    "        # Predict distance from each branch\n",
    "        dl = self.head_lidar(l_vec).squeeze(-1)   # (B,)\n",
    "        dc = self.head_cam(c_vec).squeeze(-1)\n",
    "        dr = self.head_radar(r_vec).squeeze(-1)\n",
    "        df = self.head_fused(f_vec).squeeze(-1)\n",
    "        \n",
    "        # Compute uncertainty-weighted ensemble\n",
    "        sig_l = torch.exp(self.logvar_lidar)\n",
    "        sig_c = torch.exp(self.logvar_cam)\n",
    "        sig_r = torch.exp(self.logvar_radar)\n",
    "        sig_f = torch.exp(self.logvar_fused)\n",
    "        \n",
    "        # Inverse variance weighting (learned via uncertainty)\n",
    "        wl = 1.0 / (sig_l + 1e-8)\n",
    "        wc = 1.0 / (sig_c + 1e-8)\n",
    "        wr = 1.0 / (sig_r + 1e-8)\n",
    "        wf = 1.0 / (sig_f + 1e-8)\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        w_sum = wl + wc + wr + wf\n",
    "        wl_norm = wl / w_sum\n",
    "        wc_norm = wc / w_sum\n",
    "        wr_norm = wr / w_sum\n",
    "        wf_norm = wf / w_sum\n",
    "        \n",
    "        # Ensemble prediction: weighted average of all 4 branches\n",
    "        pred_ensemble = wl_norm * dl + wc_norm * dc + wr_norm * dr + wf_norm * df\n",
    "        \n",
    "        # Return ensemble + individual predictions + normalized weights (for inference/analysis)\n",
    "        return (pred_ensemble, dl, dc, dr, df,\n",
    "                wl_norm.detach(), wc_norm.detach(), wr_norm.detach(), wf_norm.detach())\n",
    "\n",
    "# Create model\n",
    "model = UTENet4BranchBEV(\n",
    "    c_lidar=BEV_CHANNELS_LIDAR, \n",
    "    c_cam=BEV_CHANNELS_CAM, \n",
    "    c_radar=BEV_CHANNELS_RADAR, \n",
    "    d_bev=DBEV\n",
    ").to(DEVICE)\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "print(f'‚úÖ Model created: {param_count:.2f}M parameters')\n",
    "print(f'   Device: {DEVICE}')\n",
    "print(f'   Primary prediction: Ensemble (weighted average of 4 branches)')\n",
    "print(f'   Weights learned via uncertainty, modifiable for weather adaptation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HYPERPARAMETERS ==========\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "DBEV = 64\n",
    "\n",
    "print(f\"Hyperparameters: LR={LR}, EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, DBEV={DBEV}, WEIGHT_DECAY={WEIGHT_DECAY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== WEATHER-AWARE WEIGHT MODULATION ==========\n",
    "\"\"\"\n",
    "Extract weather conditions from scene descriptions and compute per-sample weather modulation factors.\n",
    "This allows the model to adaptively weight branches (LIDAR, CAMERA, RADAR, FUSED) based on \n",
    "inferred driving conditions during training and inference.\n",
    "\n",
    "Example: In rainy scenes, LIDAR and CAMERA are less reliable ‚Üí reduce their weights, boost RADAR.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_weather_from_description(description):\n",
    "    \"\"\"\n",
    "    Parse scene description for weather keywords.\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys: rain, night, fog, snow\n",
    "        values: binary (0 or 1) indicating presence\n",
    "    \"\"\"\n",
    "    desc_lower = description.lower()\n",
    "    \n",
    "    weather = {\n",
    "        'rain': int(bool(re.search(r'\\brain|raining|wet|puddle\\b', desc_lower))),\n",
    "        'night': int(bool(re.search(r'\\bnight|dark|evening\\b', desc_lower))),\n",
    "        'fog': int(bool(re.search(r'\\bfog|foggy|mist\\b', desc_lower))),\n",
    "        'snow': int(bool(re.search(r'\\bsnow|snowing\\b', desc_lower))),\n",
    "    }\n",
    "    \n",
    "    return weather\n",
    "\n",
    "\n",
    "def compute_branch_weights_for_weather(weather_dict):\n",
    "    \"\"\"\n",
    "    Compute sensor reliability modulation based on weather.\n",
    "    \n",
    "    Args:\n",
    "        weather_dict: dict from extract_weather_from_description\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys 'lidar', 'camera', 'radar', 'fused'\n",
    "        values: weight multipliers (0.0 to 2.0)\n",
    "    \n",
    "    Logic:\n",
    "        - Rain: LIDAR/CAMERA unreliable ‚Üí 0.7x; RADAR reliable ‚Üí 1.3x\n",
    "        - Night: CAMERA unreliable ‚Üí 0.7x; LIDAR/RADAR ‚Üí 1.15x\n",
    "        - Fog: LIDAR slightly unreliable ‚Üí 0.8x; RADAR ‚Üí 1.2x\n",
    "        - Snow: All sensors affected ‚Üí 0.9x baseline\n",
    "        - Fused branch: always modestly boosted (1.1x) as an ensemble safeguard\n",
    "    \"\"\"\n",
    "    \n",
    "    w_lidar = 1.0\n",
    "    w_camera = 1.0\n",
    "    w_radar = 1.0\n",
    "    w_fused = 1.1  # Ensemble safeguard\n",
    "    \n",
    "    if weather_dict['rain']:\n",
    "        w_lidar *= 0.75\n",
    "        w_camera *= 0.75\n",
    "        w_radar *= 1.25\n",
    "    \n",
    "    if weather_dict['night']:\n",
    "        w_camera *= 0.70\n",
    "        w_lidar *= 1.10\n",
    "        w_radar *= 1.10\n",
    "    \n",
    "    if weather_dict['fog']:\n",
    "        w_lidar *= 0.80\n",
    "        w_radar *= 1.20\n",
    "    \n",
    "    if weather_dict['snow']:\n",
    "        w_lidar *= 0.90\n",
    "        w_camera *= 0.90\n",
    "        w_radar *= 0.95\n",
    "    \n",
    "    return {\n",
    "        'lidar': w_lidar,\n",
    "        'camera': w_camera,\n",
    "        'radar': w_radar,\n",
    "        'fused': w_fused\n",
    "    }\n",
    "\n",
    "\n",
    "# Build weather modulation map: sample_token -> weather_dict\n",
    "sample_weather_map = {}\n",
    "for scene_rec in scene:\n",
    "    scene_token = scene_rec['token']\n",
    "    desc = scene_rec.get('description', '')\n",
    "    weather = extract_weather_from_description(desc)\n",
    "    \n",
    "    # Find all samples in this scene\n",
    "    for sample_rec in samples:\n",
    "        if sample_rec.get('scene_token') == scene_token:\n",
    "            sample_token = sample_rec['token']\n",
    "            sample_weather_map[sample_token] = weather\n",
    "\n",
    "print(f\"‚úÖ Weather map built for {len(sample_weather_map)} samples\")\n",
    "\n",
    "# Show weather distribution\n",
    "weather_counts = {\n",
    "    'rain': sum(1 for w in sample_weather_map.values() if w['rain']),\n",
    "    'night': sum(1 for w in sample_weather_map.values() if w['night']),\n",
    "    'fog': sum(1 for w in sample_weather_map.values() if w['fog']),\n",
    "    'snow': sum(1 for w in sample_weather_map.values() if w['snow']),\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Weather Distribution in Dataset:\")\n",
    "for cond, count in sorted(weather_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = 100 * count / len(sample_weather_map) if sample_weather_map else 0\n",
    "    print(f\"  {cond:15s}: {count:5d} samples ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nüí° Usage: weather_dict = sample_weather_map.get(sample_token, {})\")\n",
    "print(\"         weights = compute_branch_weights_for_weather(weather_dict)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T16:22:04.550414Z",
     "iopub.status.busy": "2025-12-06T16:22:04.549905Z",
     "iopub.status.idle": "2025-12-06T16:24:59.494815Z",
     "shell.execute_reply": "2025-12-06T16:24:59.493554Z",
     "shell.execute_reply.started": "2025-12-06T16:22:04.550377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss (weight decay for stability)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "def epoch_pass(loader, train=False, epoch_idx=0, phase_name='train'):\n",
    "    \"\"\"Single epoch pass (train or eval)\"\"\"\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_true_all, y_pred_all = [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f'{phase_name} epoch {epoch_idx}', leave=False)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        bev_l, bev_c, bev_r, dist = batch\n",
    "        bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(bev_l, bev_c, bev_r)\n",
    "        pred_ensemble = torch.nan_to_num(out[0], nan=50.0, posinf=50.0, neginf=0.0)  # primary\n",
    "        pred_lidar    = torch.nan_to_num(out[1], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        pred_cam      = torch.nan_to_num(out[2], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        pred_radar    = torch.nan_to_num(out[3], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        pred_fused    = torch.nan_to_num(out[4], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "\n",
    "        # Loss: main ensemble + auxiliary individual branch losses\n",
    "        loss_ensemble = mse_loss(pred_ensemble, dist)\n",
    "        loss_l = mse_loss(pred_lidar, dist)\n",
    "        loss_c = mse_loss(pred_cam, dist)\n",
    "        loss_r = mse_loss(pred_radar, dist)\n",
    "        loss_f = mse_loss(pred_fused, dist)\n",
    "\n",
    "        # Combined loss: emphasize ensemble, regularize with branch losses\n",
    "        loss = loss_ensemble + 0.15 * (loss_l + loss_c + loss_r + loss_f)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * dist.size(0)\n",
    "        y_true_all.extend(dist.detach().cpu().numpy())\n",
    "        y_pred_all.extend(pred_ensemble.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / max(len(loader.dataset), 1)\n",
    "    y_true_all = np.nan_to_num(np.array(y_true_all), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "    y_pred_all = np.nan_to_num(np.array(y_pred_all), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "    mse = mean_squared_error(y_true_all, y_pred_all)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_all, y_pred_all)\n",
    "    r2 = r2_score(y_true_all, y_pred_all)\n",
    "\n",
    "    return {'loss': avg_loss, 'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "\n",
    "# Training loop\n",
    "best_val_mse = float('inf')\n",
    "STATE_PATH = '/kaggle/working/utenet4bev_state.pth'\n",
    "FULL_PATH = '/kaggle/working/utenet4bev_full.pth'\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üöÄ STARTING TRAINING (Ensemble BEV - Weather Adaptive)')\n",
    "print('='*60 + '\\n')\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train pass\n",
    "    train_metrics = epoch_pass(train_loader, train=True, epoch_idx=epoch, phase_name='TRAIN')\n",
    "\n",
    "    # Val pass\n",
    "    val_metrics = epoch_pass(val_loader, train=False, epoch_idx=epoch, phase_name='VAL')\n",
    "\n",
    "    # Log summary to console\n",
    "    print(\n",
    "        f\"Epoch {epoch:2d}/{EPOCHS} | Train RMSE: {train_metrics['rmse']:.4f} | \"\n",
    "        f\"Val RMSE: {val_metrics['rmse']:.4f} | R¬≤: {val_metrics['r2']:.4f} | \"\n",
    "        f\"Train MAE: {train_metrics['mae']:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Track learned uncertainties / weights for quick inspection\n",
    "    sig_l = torch.exp(model.logvar_lidar).item()\n",
    "    sig_c = torch.exp(model.logvar_cam).item()\n",
    "    sig_r = torch.exp(model.logvar_radar).item()\n",
    "    sig_f = torch.exp(model.logvar_fused).item()\n",
    "\n",
    "    wl = 1.0 / (sig_l + 1e-8)\n",
    "    wc = 1.0 / (sig_c + 1e-8)\n",
    "    wr = 1.0 / (sig_r + 1e-8)\n",
    "    wf = 1.0 / (sig_f + 1e-8)\n",
    "    w_sum = wl + wc + wr + wf\n",
    "    wl_norm = wl / w_sum\n",
    "    wc_norm = wc / w_sum\n",
    "    wr_norm = wr / w_sum\n",
    "    wf_norm = wf / w_sum\n",
    "\n",
    "    print(\n",
    "        f\"    Weights -> lidar: {wl_norm:.2f}, cam: {wc_norm:.2f}, radar: {wr_norm:.2f}, fused: {wf_norm:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Save best model\n",
    "    if val_metrics['mse'] < best_val_mse:\n",
    "        best_val_mse = val_metrics['mse']\n",
    "        torch.save(model.state_dict(), STATE_PATH)\n",
    "        torch.save(model, FULL_PATH)\n",
    "        print(f'  ‚úÖ Saved best model (Val MSE: {best_val_mse:.4f})')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ TRAINING COMPLETE')\n",
    "print('='*60)\n",
    "\n",
    "# Test pass\n",
    "print('\\nüß™ Testing on held-out set...')\n",
    "test_metrics = epoch_pass(test_loader, train=False, epoch_idx=0, phase_name='TEST')\n",
    "print(f\"Test RMSE: {test_metrics['rmse']:.4f} | Test MAE: {test_metrics['mae']:.4f} | Test R¬≤: {test_metrics['r2']:.4f}\")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üìä FINAL RESULTS')\n",
    "print('='*60)\n",
    "print(f'Best Val MSE: {best_val_mse:.4f}')\n",
    "print(f\"Test RMSE:    {test_metrics['rmse']:.4f}m\")\n",
    "print(f\"Test MAE:     {test_metrics['mae']:.4f}m\")\n",
    "print(f\"Test R¬≤:      {test_metrics['r2']:.4f}\")\n",
    "print('\\nüí° Branch weights can be modulated at inference for weather-adaptive predictions')\n",
    "print('   Example: In rain, increase RADAR weight, decrease CAMERA weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== WEATHER-AWARE TRAINING (OPTIONAL) ==========\n",
    "\"\"\"\n",
    "Example: Use weather modulation to adaptively weight branch losses during training.\n",
    "\n",
    "To enable this, modify epoch_pass() to:\n",
    "1. Look up weather condition for each sample\n",
    "2. Compute branch weight multipliers\n",
    "3. Scale individual branch losses by these weights before summing\n",
    "\n",
    "This encourages the model to learn to trust LIDAR less in rain and CAMERA less at night, \n",
    "and to rely more on robust modalities per condition.\n",
    "\"\"\"\n",
    "\n",
    "def epoch_pass_weather_aware(loader, train=False, epoch_idx=0, phase_name='train', \n",
    "                              sample_weather_map=None, use_weather=False):\n",
    "    \"\"\"\n",
    "    Single epoch pass with optional weather-aware loss weighting.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader\n",
    "        train: bool\n",
    "        epoch_idx: epoch number\n",
    "        phase_name: name for progress bar\n",
    "        sample_weather_map: dict mapping sample_token -> weather_dict\n",
    "        use_weather: if True, modulate loss weights by weather\n",
    "    \"\"\"\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'{phase_name} epoch {epoch_idx}', leave=False)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        bev_l, bev_c, bev_r, dist = batch\n",
    "        bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(bev_l, bev_c, bev_r)\n",
    "        pred_ensemble = torch.nan_to_num(out[0], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        pred_lidar    = torch.nan_to_num(out[1], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        pred_cam      = torch.nan_to_num(out[2], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        pred_radar    = torch.nan_to_num(out[3], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        pred_fused    = torch.nan_to_num(out[4], nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        \n",
    "        # Compute branch losses\n",
    "        loss_ensemble = mse_loss(pred_ensemble, dist)\n",
    "        loss_l = mse_loss(pred_lidar, dist)\n",
    "        loss_c = mse_loss(pred_cam, dist)\n",
    "        loss_r = mse_loss(pred_radar, dist)\n",
    "        loss_f = mse_loss(pred_fused, dist)\n",
    "        \n",
    "        # (Optional) Apply weather modulation\n",
    "        if use_weather and sample_weather_map:\n",
    "            # Example: sample tokens would come from the dataset\n",
    "            # For now, use average weather weights across batch\n",
    "            weather_list = [sample_weather_map.get(tok, {}) for tok in loader.dataset.sample_tokens[step*BATCH_SIZE:(step+1)*BATCH_SIZE]]\n",
    "            \n",
    "            if weather_list:\n",
    "                avg_weights = {\n",
    "                    'lidar': np.mean([compute_branch_weights_for_weather(w)['lidar'] for w in weather_list]),\n",
    "                    'camera': np.mean([compute_branch_weights_for_weather(w)['camera'] for w in weather_list]),\n",
    "                    'radar': np.mean([compute_branch_weights_for_weather(w)['radar'] for w in weather_list]),\n",
    "                    'fused': np.mean([compute_branch_weights_for_weather(w)['fused'] for w in weather_list]),\n",
    "                }\n",
    "                \n",
    "                loss_l = loss_l * avg_weights['lidar']\n",
    "                loss_c = loss_c * avg_weights['camera']\n",
    "                loss_r = loss_r * avg_weights['radar']\n",
    "                loss_f = loss_f * avg_weights['fused']\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = loss_ensemble + 0.15 * (loss_l + loss_c + loss_r + loss_f)\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * dist.size(0)\n",
    "        y_true_all.extend(dist.detach().cpu().numpy())\n",
    "        y_pred_all.extend(pred_ensemble.detach().cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Metrics\n",
    "    avg_loss = total_loss / max(len(loader.dataset), 1)\n",
    "    y_true_all = np.nan_to_num(np.array(y_true_all), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "    y_pred_all = np.nan_to_num(np.array(y_pred_all), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "    mse = mean_squared_error(y_true_all, y_pred_all)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_all, y_pred_all)\n",
    "    r2 = r2_score(y_true_all, y_pred_all)\n",
    "    \n",
    "    return {'loss': avg_loss, 'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "\n",
    "print(\"‚úÖ WEATHER-AWARE TRAINING FUNCTION READY\")\n",
    "print(\"\\nüí° To use: Call epoch_pass_weather_aware(..., use_weather=True, sample_weather_map=sample_weather_map)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========== OPTIONAL: WEATHER-ADAPTIVE INFERENCE ==========\n",
    "# # Demonstrates how to use weather conditions to modulate branch weights\n",
    "\n",
    "# def predict_with_weather_adaptation(model, bev_lidar, bev_cam, bev_radar, weather_condition='clear'):\n",
    "#     \"\"\"\n",
    "#     Predict distance with weather-adaptive ensemble weights.\n",
    "    \n",
    "#     Args:\n",
    "#         model: trained UTENet4BranchBEV\n",
    "#         bev_lidar, bev_cam, bev_radar: BEV tensors (B, C, H, W)\n",
    "#         weather_condition: 'clear', 'rain', 'fog', 'night', 'snow'\n",
    "    \n",
    "#     Returns:\n",
    "#         pred: (B,) ensemble predictions\n",
    "#         weights_adapted: (B, 4) adapted weights for each branch\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Forward pass to get raw predictions and learned weights\n",
    "#         out = model(bev_lidar.to(DEVICE), bev_cam.to(DEVICE), bev_radar.to(DEVICE))\n",
    "#         pred_ensemble, pred_l, pred_c, pred_r, pred_f = out[:5]\n",
    "#         w_l, w_c, w_r, w_f = out[5:]  # learned weights from uncertainty\n",
    "        \n",
    "#         # Weather-adaptive weight modulation\n",
    "#         if weather_condition == 'clear':\n",
    "#             # Clear weather: all sensors reliable\n",
    "#             weather_weights = torch.tensor([0.25, 0.25, 0.25, 0.25], device=DEVICE)\n",
    "            \n",
    "#         elif weather_condition == 'rain':\n",
    "#             # Rain: LIDAR attenuated, RADAR peaks through\n",
    "#             weather_weights = torch.tensor([0.15, 0.10, 0.45, 0.30], device=DEVICE)\n",
    "            \n",
    "#         elif weather_condition == 'fog':\n",
    "#             # Fog: LIDAR poor, RADAR moderate, Fused best\n",
    "#             weather_weights = torch.tensor([0.10, 0.20, 0.30, 0.40], device=DEVICE)\n",
    "            \n",
    "#         elif weather_condition == 'night':\n",
    "#             # Night: CAMERA poor, LIDAR + RADAR good\n",
    "#             weather_weights = torch.tensor([0.35, 0.10, 0.35, 0.20], device=DEVICE)\n",
    "            \n",
    "#         elif weather_condition == 'snow':\n",
    "#             # Snow: similar to rain but RADAR slightly better\n",
    "#             weather_weights = torch.tensor([0.10, 0.05, 0.50, 0.35], device=DEVICE)\n",
    "        \n",
    "#         else:\n",
    "#             weather_weights = torch.tensor([0.25, 0.25, 0.25, 0.25], device=DEVICE)\n",
    "        \n",
    "#         # Blend learned weights with weather-adaptive weights\n",
    "#         # (can adjust blend ratio based on confidence)\n",
    "#         alpha = 0.7  # Weight for learned uncertainty vs weather\n",
    "#         combined_weights = alpha * torch.stack([w_l, w_c, w_r, w_f], dim=1) + \\\n",
    "#                           (1 - alpha) * weather_weights.unsqueeze(0)\n",
    "#         combined_weights = combined_weights / combined_weights.sum(dim=1, keepdim=True)\n",
    "        \n",
    "#         # Compute ensemble with weather-adaptive weights\n",
    "#         pred_adapted = (combined_weights[:, 0] * pred_l +\n",
    "#                        combined_weights[:, 1] * pred_c +\n",
    "#                        combined_weights[:, 2] * pred_r +\n",
    "#                        combined_weights[:, 3] * pred_f)\n",
    "        \n",
    "#         return pred_adapted.cpu(), combined_weights.detach().cpu()\n",
    "\n",
    "# # Example usage (uncomment to test)\n",
    "# # print(\"\\nüåßÔ∏è  WEATHER-ADAPTIVE INFERENCE EXAMPLE\")\n",
    "# # print(\"=\"*60)\n",
    "# # \n",
    "# # # Get a batch from test set\n",
    "# # test_batch = next(iter(test_loader))\n",
    "# # bev_l_test, bev_c_test, bev_r_test, dist_test = test_batch\n",
    "# # \n",
    "# # for weather in ['clear', 'rain', 'fog', 'night', 'snow']:\n",
    "# #     pred, weights = predict_with_weather_adaptation(\n",
    "# #         model, bev_l_test, bev_c_test, bev_r_test, weather\n",
    "# #     )\n",
    "# #     avg_weights = weights.mean(dim=0)\n",
    "# #     print(f\"\\n{weather.upper():8} | L:{avg_weights[0]:.2f} C:{avg_weights[1]:.2f} \" +\n",
    "# #           f\"R:{avg_weights[2]:.2f} F:{avg_weights[3]:.2f} | Pred: {pred[0]:.2f}m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== WEATHER-ADAPTIVE INFERENCE ==========\n",
    "\"\"\"\n",
    "At inference time, you can dynamically adjust branch weights based on detected weather,\n",
    "overriding the learned logvar weights to favor more robust sensors in bad conditions.\n",
    "\n",
    "Example: In rain, manually boost RADAR weight and reduce LIDAR/CAMERA to improve robustness.\n",
    "\"\"\"\n",
    "\n",
    "def predict_with_weather_adaptation(model, bev_l, bev_c, bev_r, weather_dict=None, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Run inference and optionally apply weather-adaptive branch weighting.\n",
    "    \n",
    "    Args:\n",
    "        model: UTENet4BranchBEV\n",
    "        bev_l, bev_c, bev_r: BEV tensors\n",
    "        weather_dict: dict from extract_weather_from_description (or None for learned weights only)\n",
    "        device: torch device\n",
    "    \n",
    "    Returns:\n",
    "        pred_ensemble: final prediction\n",
    "        individual_preds: (pred_lidar, pred_cam, pred_radar, pred_fused)\n",
    "        active_weights: (w_lidar, w_cam, w_radar, w_fused) used in prediction\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Standard forward pass (get learned logvar weights)\n",
    "        out = model(bev_l.unsqueeze(0).to(device), \n",
    "                   bev_c.unsqueeze(0).to(device), \n",
    "                   bev_r.unsqueeze(0).to(device))\n",
    "        \n",
    "        pred_ensemble_learned = out[0].squeeze().item()\n",
    "        pred_lidar = out[1].squeeze().item()\n",
    "        pred_cam = out[2].squeeze().item()\n",
    "        pred_radar = out[3].squeeze().item()\n",
    "        pred_fused = out[4].squeeze().item()\n",
    "        w_learned = out[5:]  # Learned weights\n",
    "        \n",
    "        # If weather provided, compute adaptive weights\n",
    "        if weather_dict:\n",
    "            weather_mults = compute_branch_weights_for_weather(weather_dict)\n",
    "            \n",
    "            # Recompute ensemble with weather-modulated weights\n",
    "            sig_l = torch.exp(model.logvar_lidar).item()\n",
    "            sig_c = torch.exp(model.logvar_cam).item()\n",
    "            sig_r = torch.exp(model.logvar_radar).item()\n",
    "            sig_f = torch.exp(model.logvar_fused).item()\n",
    "            \n",
    "            wl = (1.0 / (sig_l + 1e-8)) * weather_mults['lidar']\n",
    "            wc = (1.0 / (sig_c + 1e-8)) * weather_mults['camera']\n",
    "            wr = (1.0 / (sig_r + 1e-8)) * weather_mults['radar']\n",
    "            wf = (1.0 / (sig_f + 1e-8)) * weather_mults['fused']\n",
    "            \n",
    "            w_sum = wl + wc + wr + wf\n",
    "            wl_norm = wl / w_sum\n",
    "            wc_norm = wc / w_sum\n",
    "            wr_norm = wr / w_sum\n",
    "            wf_norm = wf / w_sum\n",
    "            \n",
    "            pred_ensemble_adaptive = wl_norm * pred_lidar + wc_norm * pred_cam + wr_norm * pred_radar + wf_norm * pred_fused\n",
    "            \n",
    "            return {\n",
    "                'prediction_learned': pred_ensemble_learned,\n",
    "                'prediction_adaptive': pred_ensemble_adaptive,\n",
    "                'individual': {\n",
    "                    'lidar': pred_lidar,\n",
    "                    'camera': pred_cam,\n",
    "                    'radar': pred_radar,\n",
    "                    'fused': pred_fused\n",
    "                },\n",
    "                'weights_learned': {\n",
    "                    'lidar': 1.0 / (torch.exp(model.logvar_lidar).item() + 1e-8),\n",
    "                    'camera': 1.0 / (torch.exp(model.logvar_cam).item() + 1e-8),\n",
    "                    'radar': 1.0 / (torch.exp(model.logvar_radar).item() + 1e-8),\n",
    "                    'fused': 1.0 / (torch.exp(model.logvar_fused).item() + 1e-8),\n",
    "                },\n",
    "                'weights_adaptive': {\n",
    "                    'lidar': wl_norm,\n",
    "                    'camera': wc_norm,\n",
    "                    'radar': wr_norm,\n",
    "                    'fused': wf_norm,\n",
    "                },\n",
    "                'weather': weather_dict\n",
    "            }\n",
    "        else:\n",
    "            # No weather: use learned weights only\n",
    "            sig_l = torch.exp(model.logvar_lidar).item()\n",
    "            sig_c = torch.exp(model.logvar_cam).item()\n",
    "            sig_r = torch.exp(model.logvar_radar).item()\n",
    "            sig_f = torch.exp(model.logvar_fused).item()\n",
    "            \n",
    "            wl = 1.0 / (sig_l + 1e-8)\n",
    "            wc = 1.0 / (sig_c + 1e-8)\n",
    "            wr = 1.0 / (sig_r + 1e-8)\n",
    "            wf = 1.0 / (sig_f + 1e-8)\n",
    "            w_sum = wl + wc + wr + wf\n",
    "            \n",
    "            return {\n",
    "                'prediction': pred_ensemble_learned,\n",
    "                'individual': {\n",
    "                    'lidar': pred_lidar,\n",
    "                    'camera': pred_cam,\n",
    "                    'radar': pred_radar,\n",
    "                    'fused': pred_fused\n",
    "                },\n",
    "                'weights': {\n",
    "                    'lidar': wl / w_sum,\n",
    "                    'camera': wc / w_sum,\n",
    "                    'radar': wr / w_sum,\n",
    "                    'fused': wf / w_sum,\n",
    "                },\n",
    "                'weather': None\n",
    "            }\n",
    "\n",
    "\n",
    "print(\"‚úÖ WEATHER-ADAPTIVE INFERENCE READY\")\n",
    "print(\"\\nüí° Usage:\")\n",
    "print(\"   result = predict_with_weather_adaptation(model, bev_l, bev_c, bev_r, weather_dict)\")\n",
    "print(\"   print(f'Learned prediction: {result[\\\"prediction_learned\\\"]:.2f}m')\")\n",
    "print(\"   print(f'Adaptive prediction: {result[\\\"prediction_adaptive\\\"]:.2f}m')\")\n",
    "print(\"   print(f'Weights (adaptive): {result[\\\"weights_adaptive\\\"]}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== WEATHER-AWARE vs BASELINE COMPARISON ==========\n",
    "\"\"\"\n",
    "Compare model performance with and without weather-adaptive weighting.\n",
    "This evaluates whether dynamically adjusting branch weights based on weather improves predictions.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üå¶Ô∏è  WEATHER-AWARE EVALUATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(STATE_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation function for both modes\n",
    "def evaluate_with_weather_modes(loader, dataset_name=\"test\"):\n",
    "    \"\"\"\n",
    "    Evaluate model in two modes:\n",
    "    1. Baseline: Use learned weights only (original ensemble)\n",
    "    2. Weather-Adaptive: Modulate weights by detected weather conditions\n",
    "    \n",
    "    Returns:\n",
    "        dict with results for both modes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Storage for predictions\n",
    "    baseline_preds = []\n",
    "    adaptive_preds = []\n",
    "    true_labels = []\n",
    "    weather_conditions_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(loader, desc=f\"Evaluating {dataset_name}\")):\n",
    "            bev_l, bev_c, bev_r, dist = batch\n",
    "            bev_l = bev_l.to(DEVICE)\n",
    "            bev_c = bev_c.to(DEVICE)\n",
    "            bev_r = bev_r.to(DEVICE)\n",
    "            \n",
    "            # Get sample tokens for this batch\n",
    "            start_idx = batch_idx * loader.batch_size\n",
    "            end_idx = min(start_idx + loader.batch_size, len(loader.dataset))\n",
    "            batch_tokens = loader.dataset.sample_tokens[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass (baseline predictions)\n",
    "            out = model(bev_l, bev_c, bev_r)\n",
    "            pred_baseline = out[0].cpu().numpy()  # ensemble with learned weights\n",
    "            \n",
    "            pred_lidar = out[1].cpu().numpy()\n",
    "            pred_cam = out[2].cpu().numpy()\n",
    "            pred_radar = out[3].cpu().numpy()\n",
    "            pred_fused = out[4].cpu().numpy()\n",
    "            \n",
    "            # Get learned weights\n",
    "            sig_l = torch.exp(model.logvar_lidar).item()\n",
    "            sig_c = torch.exp(model.logvar_cam).item()\n",
    "            sig_r = torch.exp(model.logvar_radar).item()\n",
    "            sig_f = torch.exp(model.logvar_fused).item()\n",
    "            \n",
    "            # Compute weather-adaptive predictions for each sample\n",
    "            pred_adaptive_batch = []\n",
    "            for i, token in enumerate(batch_tokens):\n",
    "                weather = sample_weather_map.get(token, {})\n",
    "                weather_conditions_list.append(weather)\n",
    "                \n",
    "                if weather:\n",
    "                    # Get weather modulation factors\n",
    "                    weather_mults = compute_branch_weights_for_weather(weather)\n",
    "                    \n",
    "                    # Modulate inverse-variance weights by weather\n",
    "                    wl = (1.0 / (sig_l + 1e-8)) * weather_mults['lidar']\n",
    "                    wc = (1.0 / (sig_c + 1e-8)) * weather_mults['camera']\n",
    "                    wr = (1.0 / (sig_r + 1e-8)) * weather_mults['radar']\n",
    "                    wf = (1.0 / (sig_f + 1e-8)) * weather_mults['fused']\n",
    "                    \n",
    "                    # Normalize\n",
    "                    w_sum = wl + wc + wr + wf\n",
    "                    wl_norm = wl / w_sum\n",
    "                    wc_norm = wc / w_sum\n",
    "                    wr_norm = wr / w_sum\n",
    "                    wf_norm = wf / w_sum\n",
    "                    \n",
    "                    # Recompute ensemble with weather-modulated weights\n",
    "                    pred_adaptive = (wl_norm * pred_lidar[i] + \n",
    "                                   wc_norm * pred_cam[i] + \n",
    "                                   wr_norm * pred_radar[i] + \n",
    "                                   wf_norm * pred_fused[i])\n",
    "                    pred_adaptive_batch.append(pred_adaptive)\n",
    "                else:\n",
    "                    # No weather: use baseline\n",
    "                    pred_adaptive_batch.append(pred_baseline[i])\n",
    "            \n",
    "            baseline_preds.extend(pred_baseline)\n",
    "            adaptive_preds.extend(pred_adaptive_batch)\n",
    "            true_labels.extend(dist.cpu().numpy())\n",
    "    \n",
    "    # Convert to arrays\n",
    "    baseline_preds = np.array(baseline_preds)\n",
    "    adaptive_preds = np.array(adaptive_preds)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Compute metrics for both modes\n",
    "    baseline_rmse = np.sqrt(mean_squared_error(true_labels, baseline_preds))\n",
    "    baseline_mae = mean_absolute_error(true_labels, baseline_preds)\n",
    "    baseline_r2 = r2_score(true_labels, baseline_preds)\n",
    "    \n",
    "    adaptive_rmse = np.sqrt(mean_squared_error(true_labels, adaptive_preds))\n",
    "    adaptive_mae = mean_absolute_error(true_labels, adaptive_preds)\n",
    "    adaptive_r2 = r2_score(true_labels, adaptive_preds)\n",
    "    \n",
    "    # Breakdown by weather condition\n",
    "    weather_breakdown = {}\n",
    "    for cond in ['rain', 'night', 'fog', 'snow']:\n",
    "        # Find samples with this condition\n",
    "        indices = [i for i, w in enumerate(weather_conditions_list) if w.get(cond, 0) == 1]\n",
    "        \n",
    "        if len(indices) > 0:\n",
    "            cond_true = true_labels[indices]\n",
    "            cond_baseline = baseline_preds[indices]\n",
    "            cond_adaptive = adaptive_preds[indices]\n",
    "            \n",
    "            weather_breakdown[cond] = {\n",
    "                'count': len(indices),\n",
    "                'baseline_rmse': np.sqrt(mean_squared_error(cond_true, cond_baseline)),\n",
    "                'adaptive_rmse': np.sqrt(mean_squared_error(cond_true, cond_adaptive)),\n",
    "                'baseline_mae': mean_absolute_error(cond_true, cond_baseline),\n",
    "                'adaptive_mae': mean_absolute_error(cond_true, cond_adaptive),\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'baseline': {'rmse': baseline_rmse, 'mae': baseline_mae, 'r2': baseline_r2},\n",
    "        'adaptive': {'rmse': adaptive_rmse, 'mae': adaptive_mae, 'r2': adaptive_r2},\n",
    "        'weather_breakdown': weather_breakdown,\n",
    "        'predictions': {\n",
    "            'baseline': baseline_preds,\n",
    "            'adaptive': adaptive_preds,\n",
    "            'true': true_labels\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Run comparison on test set\n",
    "results = evaluate_with_weather_modes(test_loader, \"test\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä OVERALL RESULTS (Test Set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Mode':<20} {'RMSE (m)':<15} {'MAE (m)':<15} {'R¬≤':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Baseline':<20} {results['baseline']['rmse']:<15.4f} {results['baseline']['mae']:<15.4f} {results['baseline']['r2']:<10.4f}\")\n",
    "print(f\"{'Weather-Adaptive':<20} {results['adaptive']['rmse']:<15.4f} {results['adaptive']['mae']:<15.4f} {results['adaptive']['r2']:<10.4f}\")\n",
    "\n",
    "# Compute improvement\n",
    "rmse_improvement = ((results['baseline']['rmse'] - results['adaptive']['rmse']) / results['baseline']['rmse']) * 100\n",
    "mae_improvement = ((results['baseline']['mae'] - results['adaptive']['mae']) / results['baseline']['mae']) * 100\n",
    "\n",
    "print(f\"\\n{'Improvement':<20} {rmse_improvement:>14.2f}% {mae_improvement:>14.2f}%\")\n",
    "\n",
    "# Weather-specific breakdown\n",
    "if results['weather_breakdown']:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üå¶Ô∏è  WEATHER-SPECIFIC PERFORMANCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for cond, metrics in sorted(results['weather_breakdown'].items(), key=lambda x: x[1]['count'], reverse=True):\n",
    "        if metrics['count'] > 0:\n",
    "            rmse_delta = metrics['baseline_rmse'] - metrics['adaptive_rmse']\n",
    "            mae_delta = metrics['baseline_mae'] - metrics['adaptive_mae']\n",
    "            \n",
    "            print(f\"\\n{cond.upper()} ({metrics['count']} samples):\")\n",
    "            print(f\"  Baseline:  RMSE={metrics['baseline_rmse']:.4f}m  MAE={metrics['baseline_mae']:.4f}m\")\n",
    "            print(f\"  Adaptive:  RMSE={metrics['adaptive_rmse']:.4f}m  MAE={metrics['adaptive_mae']:.4f}m\")\n",
    "            print(f\"  Delta:     RMSE={rmse_delta:+.4f}m  MAE={mae_delta:+.4f}m  {'‚úÖ Better' if rmse_delta > 0 else '‚ùå Worse'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ COMPARISON COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BRANCH PREDICTION VISUALIZATION ==========\n",
    "\"\"\"\n",
    "Visualize predictions from all 4 branches alongside ground truth.\n",
    "Shows how each branch (LIDAR, CAMERA, RADAR, FUSED) individually predicts distance.\n",
    "\"\"\"\n",
    "\n",
    "def visualize_branch_predictions(model, loader, num_samples=6, figsize=(16, 12)):\n",
    "    \"\"\"\n",
    "    Create overlaid visualization of predictions from all 4 branches.\n",
    "    \n",
    "    Args:\n",
    "        model: trained UTENet4BranchBEV\n",
    "        loader: DataLoader (train, val, or test)\n",
    "        num_samples: number of samples to visualize\n",
    "        figsize: matplotlib figure size\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions_data = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if len(predictions_data) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            bev_l, bev_c, bev_r, dist = batch\n",
    "            bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(bev_l, bev_c, bev_r)\n",
    "            pred_ensemble = np.atleast_1d(out[0].cpu().numpy()).flatten()\n",
    "            pred_l = np.atleast_1d(out[1].cpu().numpy()).flatten()\n",
    "            pred_c = np.atleast_1d(out[2].cpu().numpy()).flatten()\n",
    "            pred_r = np.atleast_1d(out[3].cpu().numpy()).flatten()\n",
    "            pred_f = np.atleast_1d(out[4].cpu().numpy()).flatten()\n",
    "            dist_gt = np.atleast_1d(dist.cpu().numpy()).flatten()\n",
    "            \n",
    "            # Get weights - flatten them (handles both scalar and batch cases)\n",
    "            w_l = np.atleast_1d(out[5].cpu().numpy()).flatten()\n",
    "            w_c = np.atleast_1d(out[6].cpu().numpy()).flatten()\n",
    "            w_r = np.atleast_1d(out[7].cpu().numpy()).flatten()\n",
    "            w_f = np.atleast_1d(out[8].cpu().numpy()).flatten()\n",
    "            \n",
    "            # Store for each sample in batch\n",
    "            for j in range(len(dist_gt)):\n",
    "                predictions_data.append({\n",
    "                    'gt': dist_gt[j],\n",
    "                    'ensemble': pred_ensemble[j],\n",
    "                    'lidar': pred_l[j],\n",
    "                    'camera': pred_c[j],\n",
    "                    'radar': pred_r[j],\n",
    "                    'fused': pred_f[j],\n",
    "                    'weight_l': w_l[j] if j < len(w_l) else w_l[0],\n",
    "                    'weight_c': w_c[j] if j < len(w_c) else w_c[0],\n",
    "                    'weight_r': w_r[j] if j < len(w_r) else w_r[0],\n",
    "                    'weight_f': w_f[j] if j < len(w_f) else w_f[0],\n",
    "                })\n",
    "    \n",
    "    # Create figure with subplots (one per sample)\n",
    "    nrows = (num_samples + 1) // 2\n",
    "    fig, axes = plt.subplots(nrows, 2, figsize=figsize)\n",
    "    if nrows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each sample\n",
    "    for idx, data in enumerate(predictions_data[:num_samples]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Predictions\n",
    "        branches = ['LIDAR', 'CAMERA', 'RADAR', 'FUSED', 'ENSEMBLE']\n",
    "        preds = [data['lidar'], data['camera'], data['radar'], data['fused'], data['ensemble']]\n",
    "        weights = [data['weight_l'], data['weight_c'], data['weight_r'], data['weight_f'], 1.0]\n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#2ECC71']\n",
    "        \n",
    "        # Create bar chart with error indicators\n",
    "        x_pos = np.arange(len(branches))\n",
    "        bars = ax.bar(x_pos, preds, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "        \n",
    "        # Add ground truth horizontal line\n",
    "        ax.axhline(y=data['gt'], color='red', linestyle='--', linewidth=3, label=f\"Ground Truth: {data['gt']:.2f}m\")\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, pred, weight) in enumerate(zip(bars, preds, weights)):\n",
    "            error = abs(pred - data['gt'])\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                   f\"{pred:.2f}m\\n(w:{weight:.2f})\", \n",
    "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_ylabel('Distance (m)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'Sample {idx+1} - Branch Predictions vs Ground Truth', \n",
    "                    fontsize=12, fontweight='bold', pad=10)\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(branches, fontsize=10)\n",
    "        ax.legend(fontsize=10, loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "        ax.set_ylim(0, max(max(preds), data['gt']) * 1.15)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(predictions_data), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, predictions_data\n",
    "\n",
    "# Run visualization on test set\n",
    "print(\"üé® GENERATING BRANCH PREDICTION VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "fig, pred_data = visualize_branch_predictions(model, test_loader, num_samples=6)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Sample Statistics from Visualization:\")\n",
    "for i, data in enumerate(pred_data[:3]):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Ground Truth:  {data['gt']:.2f}m\")\n",
    "    print(f\"  LIDAR pred:    {data['lidar']:.2f}m (error: {abs(data['lidar']-data['gt']):.2f}m, weight: {data['weight_l']:.3f})\")\n",
    "    print(f\"  CAMERA pred:   {data['camera']:.2f}m (error: {abs(data['camera']-data['gt']):.2f}m, weight: {data['weight_c']:.3f})\")\n",
    "    print(f\"  RADAR pred:    {data['radar']:.2f}m (error: {abs(data['radar']-data['gt']):.2f}m, weight: {data['weight_r']:.3f})\")\n",
    "    print(f\"  FUSED pred:    {data['fused']:.2f}m (error: {abs(data['fused']-data['gt']):.2f}m, weight: {data['weight_f']:.3f})\")\n",
    "    print(f\"  ENSEMBLE pred: {data['ensemble']:.2f}m (error: {abs(data['ensemble']-data['gt']):.2f}m)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BRANCH PERFORMANCE COMPARISON ==========\n",
    "\"\"\"\n",
    "Comprehensive metrics comparing all 4 branches across entire test set.\n",
    "Shows which branches are most reliable and their error distributions.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_all_branches(model, loader, dataset_name='test'):\n",
    "    \"\"\"\n",
    "    Compute metrics for each branch individually across entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: trained UTENet4BranchBEV\n",
    "        loader: DataLoader\n",
    "        dataset_name: name for output\n",
    "    \n",
    "    Returns:\n",
    "        dict with metrics for each branch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    branches_preds = {\n",
    "        'lidar': [],\n",
    "        'camera': [],\n",
    "        'radar': [],\n",
    "        'fused': [],\n",
    "        'ensemble': []\n",
    "    }\n",
    "    ground_truth = []\n",
    "    all_weights = {\n",
    "        'lidar': [],\n",
    "        'camera': [],\n",
    "        'radar': [],\n",
    "        'fused': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f'Evaluating {dataset_name}', leave=True)\n",
    "        for batch in pbar:\n",
    "            bev_l, bev_c, bev_r, dist = batch\n",
    "            bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(bev_l, bev_c, bev_r)\n",
    "            pred_ensemble = np.atleast_1d(out[0].cpu().numpy()).flatten()\n",
    "            pred_l = np.atleast_1d(out[1].cpu().numpy()).flatten()\n",
    "            pred_c = np.atleast_1d(out[2].cpu().numpy()).flatten()\n",
    "            pred_r = np.atleast_1d(out[3].cpu().numpy()).flatten()\n",
    "            pred_f = np.atleast_1d(out[4].cpu().numpy()).flatten()\n",
    "            w_l = np.atleast_1d(out[5].cpu().numpy()).flatten()\n",
    "            w_c = np.atleast_1d(out[6].cpu().numpy()).flatten()\n",
    "            w_r = np.atleast_1d(out[7].cpu().numpy()).flatten()\n",
    "            w_f = np.atleast_1d(out[8].cpu().numpy()).flatten()\n",
    "            dist_np = np.atleast_1d(dist.cpu().numpy()).flatten()\n",
    "            \n",
    "            branches_preds['lidar'].extend(pred_l)\n",
    "            branches_preds['camera'].extend(pred_c)\n",
    "            branches_preds['radar'].extend(pred_r)\n",
    "            branches_preds['fused'].extend(pred_f)\n",
    "            branches_preds['ensemble'].extend(pred_ensemble)\n",
    "            ground_truth.extend(dist_np)\n",
    "            \n",
    "            all_weights['lidar'].extend(w_l)\n",
    "            all_weights['camera'].extend(w_c)\n",
    "            all_weights['radar'].extend(w_r)\n",
    "            all_weights['fused'].extend(w_f)\n",
    "    \n",
    "    # Compute metrics for each branch\n",
    "    results = {}\n",
    "    for branch_name, preds in branches_preds.items():\n",
    "        preds = np.nan_to_num(np.array(preds), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        ground_truth_arr = np.nan_to_num(np.array(ground_truth), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(ground_truth_arr, preds))\n",
    "        mae = mean_absolute_error(ground_truth_arr, preds)\n",
    "        r2 = r2_score(ground_truth_arr, preds)\n",
    "        \n",
    "        results[branch_name] = {\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'predictions': preds,\n",
    "        }\n",
    "    \n",
    "    # Add average weight info for non-ensemble branches\n",
    "    for branch in ['lidar', 'camera', 'radar', 'fused']:\n",
    "        results[branch]['avg_weight'] = np.mean(all_weights[branch])\n",
    "    \n",
    "    return results, ground_truth_arr\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä BRANCH PERFORMANCE COMPARISON (Full Test Set)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "branch_results, gt_array = evaluate_all_branches(model, test_loader, \"test\")\n",
    "\n",
    "# Display comparison table\n",
    "print(f\"\\n{'Branch':<15} {'RMSE (m)':<12} {'MAE (m)':<12} {'R¬≤':<10} {'Avg Weight':<12}\")\n",
    "print(\"-\"*70)\n",
    "for branch in ['lidar', 'camera', 'radar', 'fused', 'ensemble']:\n",
    "    metrics = branch_results[branch]\n",
    "    rmse = metrics['rmse']\n",
    "    mae = metrics['mae']\n",
    "    r2 = metrics['r2']\n",
    "    weight = metrics.get('avg_weight', 1.0)\n",
    "    \n",
    "    print(f\"{branch.upper():<15} {rmse:<12.4f} {mae:<12.4f} {r2:<10.4f} {weight:<12.3f}\")\n",
    "\n",
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. RMSE comparison\n",
    "ax = axes[0, 0]\n",
    "branches = ['LIDAR', 'CAMERA', 'RADAR', 'FUSED', 'ENSEMBLE']\n",
    "rmses = [branch_results[b.lower()]['rmse'] for b in branches]\n",
    "colors_bars = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#2ECC71']\n",
    "bars = ax.bar(branches, rmses, color=colors_bars, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "ax.set_ylabel('RMSE (m)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('RMSE by Branch', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, rmse in zip(bars, rmses):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "           f'{rmse:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. MAE comparison\n",
    "ax = axes[0, 1]\n",
    "maes = [branch_results[b.lower()]['mae'] for b in branches]\n",
    "bars = ax.bar(branches, maes, color=colors_bars, edgecolor='black', linewidth=2, alpha=0.8)\n",
    "ax.set_ylabel('MAE (m)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('MAE by Branch', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, mae in zip(bars, maes):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "           f'{mae:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Prediction scatter (Ensemble vs GT)\n",
    "ax = axes[1, 0]\n",
    "ensemble_preds = branch_results['ensemble']['predictions']\n",
    "ax.scatter(gt_array, ensemble_preds, alpha=0.5, s=30, color='#2ECC71', edgecolor='black', linewidth=0.5)\n",
    "ax.plot([gt_array.min(), gt_array.max()], [gt_array.min(), gt_array.max()], \n",
    "        'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax.set_xlabel('Ground Truth (m)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Ensemble Prediction (m)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Ensemble: Predictions vs Ground Truth', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# 4. Error distribution\n",
    "ax = axes[1, 1]\n",
    "for branch, color in zip(['lidar', 'camera', 'radar', 'fused', 'ensemble'], colors_bars):\n",
    "    preds = branch_results[branch]['predictions']\n",
    "    errors = np.abs(preds - gt_array)\n",
    "    ax.hist(errors, bins=30, alpha=0.5, label=branch.upper(), color=color, edgecolor='black')\n",
    "ax.set_xlabel('Absolute Error (m)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Error Distribution by Branch', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ BRANCH COMPARISON COMPLETE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DETAILED BRANCH OVERLAY VISUALIZATION ==========\n",
    "\"\"\"\n",
    "Side-by-side overlay of all 4 branch predictions with learned weights.\n",
    "Shows exactly how each branch is contributing to the ensemble decision.\n",
    "\"\"\"\n",
    "\n",
    "def visualize_branch_overlays(model, loader, num_samples=8, figsize=(18, 10)):\n",
    "    \"\"\"\n",
    "    Create detailed overlaid bar charts showing all 4 branch predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: trained UTENet4BranchBEV\n",
    "        loader: DataLoader\n",
    "        num_samples: number of samples to visualize\n",
    "        figsize: figure size\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    sample_data = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if len(sample_data) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            bev_l, bev_c, bev_r, dist = batch\n",
    "            bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(bev_l, bev_c, bev_r)\n",
    "            pred_ensemble = np.atleast_1d(out[0].cpu().numpy()).flatten()\n",
    "            pred_l = np.atleast_1d(out[1].cpu().numpy()).flatten()\n",
    "            pred_c = np.atleast_1d(out[2].cpu().numpy()).flatten()\n",
    "            pred_r = np.atleast_1d(out[3].cpu().numpy()).flatten()\n",
    "            pred_f = np.atleast_1d(out[4].cpu().numpy()).flatten()\n",
    "            w_l = np.atleast_1d(out[5].cpu().numpy()).flatten()\n",
    "            w_c = np.atleast_1d(out[6].cpu().numpy()).flatten()\n",
    "            w_r = np.atleast_1d(out[7].cpu().numpy()).flatten()\n",
    "            w_f = np.atleast_1d(out[8].cpu().numpy()).flatten()\n",
    "            \n",
    "            dist_gt = np.atleast_1d(dist.cpu().numpy()).flatten()\n",
    "            \n",
    "            # Collect for each sample\n",
    "            for j in range(len(dist_gt)):\n",
    "                sample_data.append({\n",
    "                    'gt': dist_gt[j],\n",
    "                    'ensemble': pred_ensemble[j],\n",
    "                    'branches': {\n",
    "                        'LIDAR': pred_l[j],\n",
    "                        'CAMERA': pred_c[j],\n",
    "                        'RADAR': pred_r[j],\n",
    "                        'FUSED': pred_f[j]\n",
    "                    },\n",
    "                    'weights': {\n",
    "                        'LIDAR': w_l[j] if j < len(w_l) else w_l[0],\n",
    "                        'CAMERA': w_c[j] if j < len(w_c) else w_c[0],\n",
    "                        'RADAR': w_r[j] if j < len(w_r) else w_r[0],\n",
    "                        'FUSED': w_f[j] if j < len(w_f) else w_f[0]\n",
    "                    }\n",
    "                })\n",
    "    \n",
    "    # Create subplots\n",
    "    nrows = (num_samples + 3) // 4\n",
    "    fig, axes = plt.subplots(nrows, 4, figsize=figsize)\n",
    "    if nrows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors_branch = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "    \n",
    "    # Plot each sample\n",
    "    for idx, data in enumerate(sample_data[:num_samples]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        branch_names = list(data['branches'].keys())\n",
    "        branch_preds = list(data['branches'].values())\n",
    "        branch_weights = list(data['weights'].values())\n",
    "        \n",
    "        # Create stacked visualization: bars show predictions, width shows contribution\n",
    "        x = np.arange(len(branch_names))\n",
    "        bars = ax.bar(x, branch_preds, color=colors_branch, alpha=0.7, \n",
    "                     edgecolor='black', linewidth=2.5)\n",
    "        \n",
    "        # Add ground truth line\n",
    "        ax.axhline(y=data['gt'], color='red', linestyle='--', linewidth=3, \n",
    "                  label=f\"GT: {data['gt']:.2f}m\", zorder=10)\n",
    "        \n",
    "        # Add ensemble prediction marker\n",
    "        ax.axhline(y=data['ensemble'], color='green', linestyle=':', linewidth=2.5, \n",
    "                  label=f\"Ensemble: {data['ensemble']:.2f}m\", zorder=9)\n",
    "        \n",
    "        # Annotate each bar with prediction and weight\n",
    "        for i, (bar, pred, weight) in enumerate(zip(bars, branch_preds, branch_weights)):\n",
    "            height = bar.get_height()\n",
    "            error = abs(pred - data['gt'])\n",
    "            \n",
    "            # Main label: prediction value\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height/2,\n",
    "                   f\"{pred:.2f}m\",\n",
    "                   ha='center', va='center', fontsize=11, fontweight='bold', \n",
    "                   color='white', bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "            \n",
    "            # Top label: weight and error\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height + 1.5,\n",
    "                   f\"w:{weight:.2f}\\nerr:{error:.2f}m\",\n",
    "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Styling\n",
    "        max_y = max(max(branch_preds), data['gt']) * 1.25\n",
    "        ax.set_ylim(0, max_y)\n",
    "        ax.set_ylabel('Distance (m)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'Sample {idx+1}', fontsize=12, fontweight='bold', pad=8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(branch_names, fontsize=9, rotation=0)\n",
    "        ax.legend(fontsize=9, loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(sample_data), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Branch Predictions Overlay: All 4 Sensors vs Ensemble vs Ground Truth',\n",
    "                fontsize=15, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    return fig, sample_data\n",
    "\n",
    "# Generate the overlays\n",
    "print(\"\\nüé® DETAILED BRANCH OVERLAY VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "fig, overlay_data = visualize_branch_overlays(model, test_loader, num_samples=8)\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis for first few samples\n",
    "print(\"\\nüìã DETAILED SAMPLE ANALYSIS:\")\n",
    "print(\"=\"*70)\n",
    "for idx, data in enumerate(overlay_data[:5]):\n",
    "    print(f\"\\nüîç SAMPLE {idx+1}:\")\n",
    "    print(f\"   Ground Truth: {data['gt']:.2f}m\")\n",
    "    print(f\"   Ensemble:    {data['ensemble']:.2f}m (Error: {abs(data['ensemble']-data['gt']):.3f}m)\")\n",
    "    print(f\"\\n   Individual Branch Predictions:\")\n",
    "    \n",
    "    for branch in ['LIDAR', 'CAMERA', 'RADAR', 'FUSED']:\n",
    "        pred = data['branches'][branch]\n",
    "        weight = data['weights'][branch]\n",
    "        error = abs(pred - data['gt'])\n",
    "        error_pct = (error / data['gt'] * 100) if data['gt'] > 0 else 0\n",
    "        \n",
    "        # Determine if branch is better or worse than ensemble\n",
    "        ensemble_error = abs(data['ensemble'] - data['gt'])\n",
    "        if error < ensemble_error:\n",
    "            indicator = \"‚úÖ BETTER than ensemble\"\n",
    "        elif error > ensemble_error:\n",
    "            indicator = \"‚ùå WORSE than ensemble\"\n",
    "        else:\n",
    "            indicator = \"‚ûñ SAME as ensemble\"\n",
    "        \n",
    "        print(f\"      {branch:8} ‚Üí {pred:.2f}m  (weight: {weight:.3f}, error: {error:.3f}m ¬±{error_pct:.1f}%) {indicator}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BRANCH CONTRIBUTION HEATMAP ==========\n",
    "\"\"\"\n",
    "Show how much each branch is being used (by weight) and how accurate each is.\n",
    "Reveals which branches are trusted and which are down-weighted.\n",
    "\"\"\"\n",
    "\n",
    "def create_branch_analysis_heatmap(model, loader, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Create comprehensive heatmap showing branch metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: trained UTENet4BranchBEV\n",
    "        loader: DataLoader\n",
    "        figsize: figure size\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    branch_stats = {\n",
    "        'lidar': {'weights': [], 'errors': []},\n",
    "        'camera': {'weights': [], 'errors': []},\n",
    "        'radar': {'weights': [], 'errors': []},\n",
    "        'fused': {'weights': [], 'errors': []}\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Computing branch stats', leave=False):\n",
    "            bev_l, bev_c, bev_r, dist = batch\n",
    "            bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "            \n",
    "            out = model(bev_l, bev_c, bev_r)\n",
    "            pred_l = np.atleast_1d(out[1].cpu().numpy()).flatten()\n",
    "            pred_c = np.atleast_1d(out[2].cpu().numpy()).flatten()\n",
    "            pred_r = np.atleast_1d(out[3].cpu().numpy()).flatten()\n",
    "            pred_f = np.atleast_1d(out[4].cpu().numpy()).flatten()\n",
    "            w_l = np.atleast_1d(out[5].cpu().numpy()).flatten()\n",
    "            w_c = np.atleast_1d(out[6].cpu().numpy()).flatten()\n",
    "            w_r = np.atleast_1d(out[7].cpu().numpy()).flatten()\n",
    "            w_f = np.atleast_1d(out[8].cpu().numpy()).flatten()\n",
    "            \n",
    "            dist_np = np.atleast_1d(dist.cpu().numpy()).flatten()\n",
    "            \n",
    "            for i in range(len(dist_np)):\n",
    "                branch_stats['lidar']['weights'].append(w_l[i] if i < len(w_l) else w_l[0])\n",
    "                branch_stats['lidar']['errors'].append(np.abs(pred_l[i] - dist_np[i]))\n",
    "                \n",
    "                branch_stats['camera']['weights'].append(w_c[i] if i < len(w_c) else w_c[0])\n",
    "                branch_stats['camera']['errors'].append(np.abs(pred_c[i] - dist_np[i]))\n",
    "                \n",
    "                branch_stats['radar']['weights'].append(w_r[i] if i < len(w_r) else w_r[0])\n",
    "                branch_stats['radar']['errors'].append(np.abs(pred_r[i] - dist_np[i]))\n",
    "                \n",
    "                branch_stats['fused']['weights'].append(w_f[i] if i < len(w_f) else w_f[0])\n",
    "                branch_stats['fused']['errors'].append(np.abs(pred_f[i] - dist_np[i]))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # 1. Average Weight per Branch\n",
    "    ax = axes[0, 0]\n",
    "    branches = ['LIDAR', 'CAMERA', 'RADAR', 'FUSED']\n",
    "    avg_weights = [np.mean(branch_stats[b.lower()]['weights']) for b in branches]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "    bars = ax.bar(branches, avg_weights, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    ax.set_ylabel('Average Weight', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Learned Branch Weights (How Much Each Branch is Used)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, max(avg_weights) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    for bar, weight in zip(bars, avg_weights):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "               f'{weight:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Average Error per Branch\n",
    "    ax = axes[0, 1]\n",
    "    avg_errors = [np.mean(branch_stats[b.lower()]['errors']) for b in branches]\n",
    "    bars = ax.bar(branches, avg_errors, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    ax.set_ylabel('Mean Absolute Error (m)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Average Error per Branch (Accuracy)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, max(avg_errors) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    for bar, error in zip(bars, avg_errors):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "               f'{error:.3f}m', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Weight vs Error Scatter\n",
    "    ax = axes[1, 0]\n",
    "    for branch, color in zip(['lidar', 'camera', 'radar', 'fused'], colors):\n",
    "        weights = np.array(branch_stats[branch]['weights'])\n",
    "        errors = np.array(branch_stats[branch]['errors'])\n",
    "        ax.scatter(weights, errors, alpha=0.5, s=20, label=branch.upper(), color=color)\n",
    "    ax.set_xlabel('Learned Weight', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Prediction Error (m)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Branch Weight vs Prediction Error\\n(Ideal: high weight + low error)', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Weight Distribution Violin Plot\n",
    "    ax = axes[1, 1]\n",
    "    weight_data = [branch_stats[b.lower()]['weights'] for b in branches]\n",
    "    parts = ax.violinplot(weight_data, positions=np.arange(len(branches)), \n",
    "                          showmeans=True, showmedians=True)\n",
    "    ax.set_ylabel('Weight Value', fontsize=11, fontweight='bold')\n",
    "    ax.set_title('Weight Distribution per Branch (Violin Plot)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(np.arange(len(branches)))\n",
    "    ax.set_xticklabels(branches)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, branch_stats\n",
    "\n",
    "# Generate heatmap\n",
    "print(\"\\nüìä BRANCH CONTRIBUTION & ACCURACY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "fig, branch_stats = create_branch_analysis_heatmap(model, test_loader)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nüìà BRANCH STATISTICS SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Branch':<12} {'Avg Weight':<15} {'Avg Error':<15} {'Min Error':<15} {'Max Error':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for branch in ['lidar', 'camera', 'radar', 'fused']:\n",
    "    weights = np.array(branch_stats[branch]['weights'])\n",
    "    errors = np.array(branch_stats[branch]['errors'])\n",
    "    \n",
    "    print(f\"{branch.upper():<12} {np.mean(weights):<15.4f} {np.mean(errors):<15.4f} \"\n",
    "          f\"{np.min(errors):<15.4f} {np.max(errors):<15.4f}\")\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION:\")\n",
    "print(\"-\"*70)\n",
    "print(\"‚Ä¢ High Weight + Low Error = Branch is trusted and accurate\")\n",
    "print(\"‚Ä¢ High Weight + High Error = Branch is trusted but unreliable (needs retraining)\")\n",
    "print(\"‚Ä¢ Low Weight + Low Error = Branch is accurate but not used (good ensemble decision)\")\n",
    "print(\"‚Ä¢ Low Weight + High Error = Branch is unreliable and correctly down-weighted\")\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SIMPLE CAMERA + BEV VISUALIZATION ==========\n",
    "\"\"\"\n",
    "Show a single sample with:\n",
    "- CAM_FRONT image\n",
    "- LIDAR BEV count channel\n",
    "- RADAR BEV count channel\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "def load_cam_image(sample_token, fallback_size=(640, 360)):\n",
    "    \"\"\"Load CAM_FRONT image for a sample_token if available; else return a blank image.\"\"\"\n",
    "    if sample_token not in sample_to_sensor:\n",
    "        return Image.new('RGB', fallback_size, color=(30, 30, 30))\n",
    "    sensors = sample_to_sensor[sample_token]\n",
    "    if 'CAM_FRONT' not in sensors:\n",
    "        return Image.new('RGB', fallback_size, color=(30, 30, 30))\n",
    "    sd_tok = sensors['CAM_FRONT']\n",
    "    sd_rec = sd_by_token.get(sd_tok)\n",
    "    if not sd_rec:\n",
    "        return Image.new('RGB', fallback_size, color=(30, 30, 30))\n",
    "    img_path = abs_sensor_path(sd_rec)\n",
    "    if not os.path.exists(img_path):\n",
    "        return Image.new('RGB', fallback_size, color=(30, 30, 30))\n",
    "    try:\n",
    "        return Image.open(img_path).convert('RGB')\n",
    "    except Exception:\n",
    "        return Image.new('RGB', fallback_size, color=(30, 30, 30))\n",
    "\n",
    "\n",
    "def visualize_camera_and_bev(dataset, sample_idx=None, figsize=(16, 5)):\n",
    "    \"\"\"\n",
    "    Show CAM image + LIDAR count BEV + RADAR count BEV for one sample.\n",
    "    If sample_idx is None, pick a random sample.\n",
    "    \"\"\"\n",
    "    if sample_idx is None:\n",
    "        sample_idx = random.randint(0, len(dataset) - 1)\n",
    "    \n",
    "    # Get sample\n",
    "    bev_l, bev_c, bev_r, dist = dataset[sample_idx]\n",
    "    sample_token = dataset.sample_tokens[sample_idx]\n",
    "    img = load_cam_image(sample_token)\n",
    "    \n",
    "    # Extract count channels\n",
    "    lidar_count = bev_l[0].numpy() if hasattr(bev_l, 'numpy') else bev_l[0]\n",
    "    radar_count = bev_r[0].numpy() if hasattr(bev_r, 'numpy') else bev_r[0]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(f'CAM_FRONT (idx {sample_idx})')\n",
    "    \n",
    "    im1 = axes[1].imshow(lidar_count, cmap='viridis', aspect='auto')\n",
    "    axes[1].set_title('LIDAR BEV (Count)')\n",
    "    fig.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    im2 = axes[2].imshow(radar_count, cmap='plasma', aspect='auto')\n",
    "    axes[2].set_title('RADAR BEV (Count)')\n",
    "    fig.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run: simple camera + BEV visualization on one random sample\n",
    "print(\"\\nüé® SIMPLE CAMERA + BEV VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "visualize_camera_and_bev(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CAMERA + BEV PERSPECTIVE ANALYSIS ==========\n",
    "\"\"\"\n",
    "Understand why objects in camera images appear farther than BEV suggests.\n",
    "This compares camera perspective projection with BEV bird's-eye-view.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìä CAMERA vs BEV PERSPECTIVE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Camera parameters (typical nuScenes CAM_FRONT)\n",
    "CAM_INTRINSICS = {\n",
    "    'fx': 1266.4,      # focal length x\n",
    "    'fy': 1266.4,      # focal length y\n",
    "    'cx': 816.0,       # principal point x\n",
    "    'cy': 491.0,       # principal point y\n",
    "    'width': 1600,\n",
    "    'height': 900,\n",
    "}\n",
    "\n",
    "CAM_HEIGHT = 1.7  # meters above ground (approximate ego mounting height)\n",
    "IMG_WIDTH = CAM_INTRINSICS['width']\n",
    "IMG_HEIGHT = CAM_INTRINSICS['height']\n",
    "\n",
    "print(f\"\\nüé• CAMERA PROPERTIES:\")\n",
    "print(f\"  Image size: {IMG_WIDTH} x {IMG_HEIGHT}\")\n",
    "print(f\"  Focal length: {CAM_INTRINSICS['fx']:.1f}px\")\n",
    "print(f\"  Principal point: ({CAM_INTRINSICS['cx']:.1f}, {CAM_INTRINSICS['cy']:.1f})\")\n",
    "print(f\"  Camera height above ground: {CAM_HEIGHT}m\")\n",
    "\n",
    "# Field of view calculation\n",
    "fov_x = 2 * np.arctan(IMG_WIDTH / (2 * CAM_INTRINSICS['fx'])) * 180 / np.pi\n",
    "fov_y = 2 * np.arctan(IMG_HEIGHT / (2 * CAM_INTRINSICS['fy'])) * 180 / np.pi\n",
    "\n",
    "print(f\"\\nüìê FIELD OF VIEW:\")\n",
    "print(f\"  Horizontal FOV: {fov_x:.1f}¬∞\")\n",
    "print(f\"  Vertical FOV: {fov_y:.1f}¬∞\")\n",
    "\n",
    "print(f\"\\nüõ£Ô∏è  BEV COVERAGE:\")\n",
    "print(f\"  Forward range: {XRANGE[0]:.0f}m to {XRANGE[1]:.0f}m (200m total)\")\n",
    "print(f\"  Lateral range: {YRANGE[0]:.0f}m to {YRANGE[1]:.0f}m (100m total)\")\n",
    "print(f\"  Grid resolution: {RES}m/cell\")\n",
    "print(f\"  Grid size: {NX} x {NY} cells\")\n",
    "\n",
    "print(f\"\\nüí° WHY OBJECTS APPEAR FARTHER IN CAMERA:\")\n",
    "print(f\"  1. Camera perspective: Objects at 10m look ~5% into the 200m BEV range\")\n",
    "print(f\"  2. Pinhole projection: Smaller image = appears farther\")\n",
    "print(f\"  3. Camera height: Ground at 10m is below horizon, looks very distant\")\n",
    "print(f\"  4. No depth cues: Without stereo/depth, perception is ambiguous\")\n",
    "\n",
    "print(f\"\\nüìè EXAMPLE DISTANCE INTERPRETATIONS:\")\n",
    "test_distances = [5, 10, 20, 30, 50]\n",
    "for dist in test_distances:\n",
    "    percent_of_bev = (dist - XRANGE[0]) / (XRANGE[1] - XRANGE[0]) * 100\n",
    "    print(f\"  {dist:2d}m lead vehicle = {percent_of_bev:5.1f}% into forward BEV range\")\n",
    "\n",
    "print(\"\\n‚úÖ CONCLUSION: Use BEV predictions, not camera visual perception for distance.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CAMERA + FUSION + BRANCH WEIGHTS (SINGLE SAMPLE) ==========\n",
    "\"\"\"\n",
    "Show for one sample:\n",
    "- CAM_FRONT image\n",
    "- BEV fusion visualization (LIDAR + RADAR + CAMERA)\n",
    "- Branch predictions (no GT line)\n",
    "- Normalized branch weights for that sample\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def visualize_camera_fusion_and_weights(model, dataset, sample_idx=None):\n",
    "    model.eval()\n",
    "    if sample_idx is None:\n",
    "        sample_idx = random.randint(0, len(dataset) - 1)\n",
    "\n",
    "    # Grab sample\n",
    "    bev_l, bev_c, bev_r, dist = dataset[sample_idx]\n",
    "    sample_token = dataset.sample_tokens[sample_idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Batch dim\n",
    "        bev_l_b = bev_l.unsqueeze(0).to(DEVICE)\n",
    "        bev_c_b = bev_c.unsqueeze(0).to(DEVICE)\n",
    "        bev_r_b = bev_r.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(bev_l_b, bev_c_b, bev_r_b)\n",
    "        pred_ensemble, pred_l, pred_c, pred_r, pred_f = [t.cpu().item() for t in out[:5]]\n",
    "        w_l, w_c, w_r, w_f = [t.cpu().item() for t in out[5:9]]\n",
    "\n",
    "    # Camera image\n",
    "    img = load_cam_image(sample_token)\n",
    "\n",
    "    # BEVs to numpy\n",
    "    lidar_bev = bev_l.numpy() if hasattr(bev_l, 'numpy') else bev_l\n",
    "    cam_bev = bev_c.numpy() if hasattr(bev_c, 'numpy') else bev_c\n",
    "    radar_bev = bev_r.numpy() if hasattr(bev_r, 'numpy') else bev_r\n",
    "\n",
    "    # 1) Camera image\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'CAM_FRONT (sample {sample_idx})')\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Fusion visualization (uses existing helper)\n",
    "    fig_fusion = visualize_bev_fusion(lidar_bev, cam_bev, radar_bev, distance_label=None)\n",
    "    plt.show()\n",
    "\n",
    "    # 3) Predictions + Weights\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    branches = ['LIDAR', 'CAMERA', 'RADAR', 'FUSED', 'ENSEMBLE']\n",
    "    preds = [pred_l, pred_c, pred_r, pred_f, pred_ensemble]\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#2ECC71']\n",
    "    x = np.arange(len(branches))\n",
    "    bars = axes[0].bar(x, preds, color=colors, edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(branches, rotation=15)\n",
    "    axes[0].set_ylabel('Distance (m)')\n",
    "    axes[0].set_title('Branch Predictions (no GT)')\n",
    "    axes[0].grid(axis='y', alpha=0.3, linestyle=':')\n",
    "    axes[0].set_ylim(0, max(preds) * 1.25)\n",
    "    for bar, val in zip(bars, preds):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                     f\"{val:.2f}m\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    weights = [w_l, w_c, w_r, w_f]\n",
    "    branches_w = ['LIDAR', 'CAM', 'RADAR', 'FUSED']\n",
    "    xw = np.arange(len(weights))\n",
    "    bars_w = axes[1].bar(xw, weights, color=colors[:4], edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "    axes[1].set_xticks(xw)\n",
    "    axes[1].set_xticklabels(branches_w, rotation=15)\n",
    "    axes[1].set_ylabel('Weight (normalized)')\n",
    "    axes[1].set_title('Branch Weights (this sample)')\n",
    "    axes[1].grid(axis='y', alpha=0.3, linestyle=':')\n",
    "    axes[1].set_ylim(0, max(weights) * 1.25)\n",
    "    for bar, val in zip(bars_w, weights):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                     f\"{val:.3f}\", ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"‚úÖ Sample {sample_idx}: Token={sample_token[:8]}... | Ensemble={pred_ensemble:.2f}m | Weights L/C/R/F = {w_l:.3f}/{w_c:.3f}/{w_r:.3f}/{w_f:.3f}\")\n",
    "\n",
    "\n",
    "# Run on a single random sample\n",
    "print(\"\\nüé® CAMERA + FUSION + BRANCH WEIGHTS (SINGLE SAMPLE)\")\n",
    "print(\"=\"*70)\n",
    "visualize_camera_fusion_and_weights(model, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== SIMPLE SENSOR STREAM + FUSION VIEW ==========\n",
    "\"\"\"\n",
    "Minimal visualization for one sample:\n",
    "- CAM_FRONT image\n",
    "- LIDAR BEV count channel (with nonzero count + max)\n",
    "- RADAR BEV count channel (with nonzero count + max)\n",
    "- Fusion (LIDAR + RADAR count) with stats\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_sensor_stream_simple(dataset, sample_idx=None, figsize=(18, 4)):\n",
    "    if sample_idx is None:\n",
    "        sample_idx = random.randint(0, len(dataset) - 1)\n",
    "    \n",
    "    bev_l, bev_c, bev_r, dist = dataset[sample_idx]\n",
    "    sample_token = dataset.sample_tokens[sample_idx]\n",
    "    img = load_cam_image(sample_token)\n",
    "    \n",
    "    # Count channels as numpy\n",
    "    lidar_count = bev_l[0].cpu().numpy() if hasattr(bev_l, 'cpu') else bev_l[0]\n",
    "    radar_count = bev_r[0].cpu().numpy() if hasattr(bev_r, 'cpu') else bev_r[0]\n",
    "    fusion_count = lidar_count + radar_count\n",
    "    \n",
    "    def stats(arr):\n",
    "        nz = int((arr > 0).sum())\n",
    "        mx = float(arr.max()) if arr.size > 0 else 0.0\n",
    "        return nz, mx\n",
    "    lid_nz, lid_max = stats(lidar_count)\n",
    "    rad_nz, rad_max = stats(radar_count)\n",
    "    fus_nz, fus_max = stats(fusion_count)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=figsize)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(f'CAM_FRONT (idx {sample_idx})')\n",
    "    \n",
    "    im1 = axes[1].imshow(lidar_count, cmap='viridis', aspect='auto')\n",
    "    axes[1].set_title(f'LIDAR Count\\nnonzero={lid_nz}, max={lid_max:.2f}')\n",
    "    fig.colorbar(im1, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    im2 = axes[2].imshow(radar_count, cmap='plasma', aspect='auto')\n",
    "    axes[2].set_title(f'RADAR Count\\nnonzero={rad_nz}, max={rad_max:.2f}')\n",
    "    fig.colorbar(im2, ax=axes[2], fraction=0.046)\n",
    "    \n",
    "    im3 = axes[3].imshow(fusion_count, cmap='magma', aspect='auto')\n",
    "    axes[3].set_title(f'Fusion (L+R)\\nnonzero={fus_nz}, max={fus_max:.2f}')\n",
    "    fig.colorbar(im3, ax=axes[3], fraction=0.046)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nüé® SIMPLE SENSOR STREAM + FUSION VIEW\")\n",
    "print(\"=\"*70)\n",
    "visualize_sensor_stream_simple(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== WEATHER-AWARE HELPER FUNCTIONS ==========\n",
    "\n",
    "# Weather modulation factors (applied multiplicatively)\n",
    "WEATHER_MODULATION = {\n",
    "    'rain':  {'lidar': 0.7, 'camera': 0.6, 'radar': 1.2, 'fused': 1.0},\n",
    "    'night': {'lidar': 1.0, 'camera': 0.5, 'radar': 1.1, 'fused': 1.0},\n",
    "    'fog':   {'lidar': 0.6, 'camera': 0.5, 'radar': 1.3, 'fused': 1.0},\n",
    "    'snow':  {'lidar': 0.8, 'camera': 0.7, 'radar': 0.9, 'fused': 1.0}\n",
    "}\n",
    "\n",
    "\n",
    "def extract_weather_from_description(description):\n",
    "    \"\"\"\n",
    "    Extract weather conditions from scene description string.\n",
    "    \n",
    "    Args:\n",
    "        description: str, scene description (e.g., 'rainy', 'night', 'foggy')\n",
    "    \n",
    "    Returns:\n",
    "        dict: weather conditions with boolean flags for rain, night, fog, snow\n",
    "    \"\"\"\n",
    "    if not description or not isinstance(description, str):\n",
    "        return {'rain': 0, 'night': 0, 'fog': 0, 'snow': 0}\n",
    "    \n",
    "    desc_lower = description.lower()\n",
    "    \n",
    "    weather = {\n",
    "        'rain': 1 if 'rain' in desc_lower or 'wet' in desc_lower else 0,\n",
    "        'night': 1 if 'night' in desc_lower else 0,\n",
    "        'fog': 1 if 'fog' in desc_lower or 'overcast' in desc_lower else 0,\n",
    "        'snow': 1 if 'snow' in desc_lower else 0\n",
    "    }\n",
    "    \n",
    "    return weather\n",
    "\n",
    "\n",
    "def get_weather_adjusted_weights(base_weights, weather_dict):\n",
    "    \"\"\"\n",
    "    Adjust base branch weights based on weather conditions.\n",
    "    \n",
    "    Args:\n",
    "        base_weights: tuple of (w_lidar, w_camera, w_radar, w_fused) - can be floats or tensors\n",
    "        weather_dict: dict with keys 'rain', 'night', 'fog', 'snow' (binary 0/1)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (w_lidar_adj, w_camera_adj, w_radar_adj, w_fused_adj) - adjusted weights\n",
    "    \"\"\"\n",
    "    # Convert tensors to scalars if needed\n",
    "    w_l, w_c, w_r, w_f = base_weights\n",
    "    if hasattr(w_l, 'item'):\n",
    "        w_l = w_l.item()\n",
    "    if hasattr(w_c, 'item'):\n",
    "        w_c = w_c.item()\n",
    "    if hasattr(w_r, 'item'):\n",
    "        w_r = w_r.item()\n",
    "    if hasattr(w_f, 'item'):\n",
    "        w_f = w_f.item()\n",
    "    \n",
    "    # Start with base weights\n",
    "    w_l_adj = w_l\n",
    "    w_c_adj = w_c\n",
    "    w_r_adj = w_r\n",
    "    w_f_adj = w_f\n",
    "    \n",
    "    # Apply modulation factors based on active weather conditions\n",
    "    for condition, is_active in weather_dict.items():\n",
    "        if is_active and condition in WEATHER_MODULATION:\n",
    "            factors = WEATHER_MODULATION[condition]\n",
    "            w_l_adj *= factors['lidar']\n",
    "            w_c_adj *= factors['camera']\n",
    "            w_r_adj *= factors['radar']\n",
    "            w_f_adj *= factors['fused']\n",
    "    \n",
    "    # Normalize\n",
    "    w_sum = w_l_adj + w_c_adj + w_r_adj + w_f_adj + 1e-8\n",
    "    w_l_adj = w_l_adj / w_sum\n",
    "    w_c_adj = w_c_adj / w_sum\n",
    "    w_r_adj = w_r_adj / w_sum\n",
    "    w_f_adj = w_f_adj / w_sum\n",
    "    \n",
    "    return (w_l_adj, w_c_adj, w_r_adj, w_f_adj)\n",
    "\n",
    "\n",
    "print(\"‚úÖ WEATHER-AWARE HELPER FUNCTIONS LOADED\")\n",
    "print(f\"   - Weather conditions: {list(WEATHER_MODULATION.keys())}\")\n",
    "print(f\"   - Extraction & adjustment functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== WEATHER-AWARE TRAINING WITH WEIGHT TRACKING ==========\n",
    "\n",
    "# Initialize tracking dictionary\n",
    "weight_history = {'train': {}, 'val': {}}\n",
    "loss_history = {'train': [], 'val': []}\n",
    "\n",
    "\n",
    "def epoch_pass_with_weather(loader, train=False, epoch_idx=0, phase_name='train', use_weather_modulation=True):\n",
    "    \"\"\"\n",
    "    Single epoch pass with weather-aware weight modulation during training.\n",
    "    Tracks weight evolution for visualization.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader with samples and tokens\n",
    "        train: bool\n",
    "        epoch_idx: epoch number\n",
    "        phase_name: name for progress bar\n",
    "        use_weather_modulation: if True, apply weather adjustments to weights\n",
    "    \"\"\"\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    w_l_list, w_c_list, w_r_list, w_f_list = [], [], [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'{phase_name} epoch {epoch_idx}', leave=False)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        bev_l, bev_c, bev_r, dist = batch\n",
    "        batch_size = bev_l.shape[0]\n",
    "        \n",
    "        # Get sample tokens for this batch\n",
    "        start_idx = step * loader.batch_size\n",
    "        end_idx = min(start_idx + loader.batch_size, len(loader.dataset))\n",
    "        batch_tokens = loader.dataset.sample_tokens[start_idx:end_idx]\n",
    "        \n",
    "        bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(bev_l, bev_c, bev_r)\n",
    "        pred_ensemble, pred_lidar, pred_cam, pred_radar, pred_fused = out[:5]\n",
    "        w_l_base, w_c_base, w_r_base, w_f_base = [t.detach() for t in out[5:9]]\n",
    "        \n",
    "        # Apply weather modulation if enabled (during training)\n",
    "        if train and use_weather_modulation:\n",
    "            # Get weather for each sample in batch\n",
    "            adjusted_weights_list = []\n",
    "            for token in batch_tokens:\n",
    "                if token in sample_to_sensor:\n",
    "                    weather = extract_weather_from_description(\n",
    "                        sample_by_token.get(token, {}).get('scene_description', '')\n",
    "                    )\n",
    "                else:\n",
    "                    weather = {'rain': 0, 'night': 0, 'fog': 0, 'snow': 0}\n",
    "                \n",
    "                # Adjust weights based on weather\n",
    "                adj_w = get_weather_adjusted_weights(\n",
    "                    (w_l_base.mean(), w_c_base.mean(), w_r_base.mean(), w_f_base.mean()),\n",
    "                    weather\n",
    "                )\n",
    "                adjusted_weights_list.append(adj_w)\n",
    "            \n",
    "            # Use weather-adjusted weights for ensemble\n",
    "            adjusted_weights = torch.tensor(adjusted_weights_list, device=DEVICE)\n",
    "            w_l = torch.tensor([w[0] for w in adjusted_weights_list], device=DEVICE).mean()\n",
    "            w_c = torch.tensor([w[1] for w in adjusted_weights_list], device=DEVICE).mean()\n",
    "            w_r = torch.tensor([w[2] for w in adjusted_weights_list], device=DEVICE).mean()\n",
    "            w_f = torch.tensor([w[3] for w in adjusted_weights_list], device=DEVICE).mean()\n",
    "        else:\n",
    "            # Use learned weights (no weather modulation)\n",
    "            w_l, w_c, w_r, w_f = w_l_base, w_c_base, w_r_base, w_f_base\n",
    "        \n",
    "        # Normalize weights\n",
    "        w_sum = w_l + w_c + w_r + w_f\n",
    "        w_l_norm = w_l / w_sum\n",
    "        w_c_norm = w_c / w_sum\n",
    "        w_r_norm = w_r / w_sum\n",
    "        w_f_norm = w_f / w_sum\n",
    "        \n",
    "        # Track weights (convert to scalar if tensor)\n",
    "        w_l_val = w_l_norm.item() if hasattr(w_l_norm, 'item') else float(w_l_norm)\n",
    "        w_c_val = w_c_norm.item() if hasattr(w_c_norm, 'item') else float(w_c_norm)\n",
    "        w_r_val = w_r_norm.item() if hasattr(w_r_norm, 'item') else float(w_r_norm)\n",
    "        w_f_val = w_f_norm.item() if hasattr(w_f_norm, 'item') else float(w_f_norm)\n",
    "        w_l_list.append(w_l_val)\n",
    "        w_c_list.append(w_c_val)\n",
    "        w_r_list.append(w_r_val)\n",
    "        w_f_list.append(w_f_val)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_ensemble = mse_loss(pred_ensemble, dist)\n",
    "        loss_l = mse_loss(pred_lidar, dist)\n",
    "        loss_c = mse_loss(pred_cam, dist)\n",
    "        loss_r = mse_loss(pred_radar, dist)\n",
    "        loss_f = mse_loss(pred_fused, dist)\n",
    "        \n",
    "        # Apply weather-adjusted weights to branch losses (emphasize reliable branches)\n",
    "        loss_l = loss_l * w_l_norm.detach()\n",
    "        loss_c = loss_c * w_c_norm.detach()\n",
    "        loss_r = loss_r * w_r_norm.detach()\n",
    "        loss_f = loss_f * w_f_norm.detach()\n",
    "        \n",
    "        # Combined loss: ensemble + weather-weighted auxiliary branches\n",
    "        loss = loss_ensemble + 0.15 * (loss_l + loss_c + loss_r + loss_f)\n",
    "        \n",
    "        if train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * dist.size(0)\n",
    "        y_true_all.extend(dist.detach().cpu().numpy())\n",
    "        y_pred_all.extend(pred_ensemble.detach().cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    # Metrics\n",
    "    avg_loss = total_loss / max(len(loader.dataset), 1)\n",
    "    y_true_all = np.nan_to_num(np.array(y_true_all), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "    y_pred_all = np.nan_to_num(np.array(y_pred_all), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "    mse = mean_squared_error(y_true_all, y_pred_all)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_all, y_pred_all)\n",
    "    r2 = r2_score(y_true_all, y_pred_all)\n",
    "    \n",
    "    # Store weight history\n",
    "    phase = 'train' if train else 'val'\n",
    "    weight_history[phase][epoch_idx] = {\n",
    "        'lidar': np.mean(w_l_list),\n",
    "        'camera': np.mean(w_c_list),\n",
    "        'radar': np.mean(w_r_list),\n",
    "        'fused': np.mean(w_f_list),\n",
    "        'lidar_std': np.std(w_l_list),\n",
    "        'camera_std': np.std(w_c_list),\n",
    "        'radar_std': np.std(w_r_list),\n",
    "        'fused_std': np.std(w_f_list),\n",
    "    }\n",
    "    \n",
    "    if train:\n",
    "        loss_history['train'].append(avg_loss)\n",
    "    else:\n",
    "        loss_history['val'].append(avg_loss)\n",
    "    \n",
    "    return {'loss': avg_loss, 'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "\n",
    "# Training with weather-aware modulation\n",
    "best_val_mse = float('inf')\n",
    "STATE_PATH = '/kaggle/working/utenet4bev_state.pth'\n",
    "FULL_PATH = '/kaggle/working/utenet4bev_full.pth'\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('üöÄ STARTING TRAINING (Weather-Aware Weight Modulation)')\n",
    "print('='*60 + '\\n')\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train with weather modulation\n",
    "    train_metrics = epoch_pass_with_weather(train_loader, train=True, epoch_idx=epoch, \n",
    "                                            phase_name='TRAIN', use_weather_modulation=True)\n",
    "    \n",
    "    # Val without weather modulation (use learned weights only)\n",
    "    val_metrics = epoch_pass_with_weather(val_loader, train=False, epoch_idx=epoch, \n",
    "                                          phase_name='VAL', use_weather_modulation=False)\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch:2d}/{EPOCHS} | Train RMSE: {train_metrics['rmse']:.4f} | \"\n",
    "        f\"Val RMSE: {val_metrics['rmse']:.4f} | R¬≤: {val_metrics['r2']:.4f} | \"\n",
    "        f\"Train MAE: {train_metrics['mae']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    # Checkpoint best model\n",
    "    if val_metrics['mse'] < best_val_mse:\n",
    "        best_val_mse = val_metrics['mse']\n",
    "        torch.save(model.state_dict(), STATE_PATH)\n",
    "        torch.save(model, FULL_PATH)\n",
    "        print(f\"   ‚úÖ New best validation MSE: {best_val_mse:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE WITH WEATHER-AWARE MODULATION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== VISUALIZING BRANCH WEIGHT EVOLUTION DURING TRAINING ==========\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Branch Weight Evolution During Training (Weather-Aware)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Extract epoch numbers and weight values\n",
    "epochs_train = sorted(weight_history['train'].keys())\n",
    "epochs_val = sorted(weight_history['val'].keys())\n",
    "\n",
    "# 1. Training Weights with Error Bands\n",
    "ax = axes[0, 0]\n",
    "branches = ['lidar', 'camera', 'radar', 'fused']\n",
    "colors = {'lidar': '#FF6B6B', 'camera': '#4ECDC4', 'radar': '#45B7D1', 'fused': '#FFA07A'}\n",
    "\n",
    "for branch in branches:\n",
    "    means = [weight_history['train'][e][branch] for e in epochs_train]\n",
    "    stds = [weight_history['train'][e][f'{branch}_std'] for e in epochs_train]\n",
    "    ax.plot(epochs_train, means, marker='o', label=branch.upper(), color=colors[branch], linewidth=2)\n",
    "    ax.fill_between(epochs_train, \n",
    "                     np.array(means) - np.array(stds),\n",
    "                     np.array(means) + np.array(stds),\n",
    "                     alpha=0.2, color=colors[branch])\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Weight', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Training Weights (Mean ¬± Std)', fontsize=12)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(epochs_train)\n",
    "\n",
    "# 2. Validation Weights (no std since validation uses learned weights only)\n",
    "ax = axes[0, 1]\n",
    "for branch in branches:\n",
    "    means = [weight_history['val'][e][branch] for e in epochs_val]\n",
    "    ax.plot(epochs_val, means, marker='s', label=branch.upper(), color=colors[branch], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Weight', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Validation Weights (Learned Only)', fontsize=12)\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(epochs_val)\n",
    "\n",
    "# 3. Weight Distribution at Each Epoch (Train) - Stacked Area\n",
    "ax = axes[1, 0]\n",
    "all_means_train = {branch: [weight_history['train'][e][branch] for e in epochs_train] for branch in branches}\n",
    "ax.stackplot(epochs_train,\n",
    "             all_means_train['lidar'],\n",
    "             all_means_train['camera'],\n",
    "             all_means_train['radar'],\n",
    "             all_means_train['fused'],\n",
    "             labels=[b.upper() for b in branches],\n",
    "             colors=[colors[b] for b in branches],\n",
    "             alpha=0.7)\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Normalized Weight', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Stacked Weight Distribution (Training)', fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_xticks(epochs_train)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# 4. Training vs Validation Loss with Weight Divergence\n",
    "ax = axes[1, 1]\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "# Plot losses on primary axis\n",
    "ax.plot(range(1, len(loss_history['train'])+1), loss_history['train'], \n",
    "        marker='o', label='Train Loss', color='#2E86C1', linewidth=2)\n",
    "ax.plot(range(1, len(loss_history['val'])+1), loss_history['val'], \n",
    "        marker='s', label='Val Loss', color='#E74C3C', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Loss', fontsize=11, fontweight='bold', color='#2E86C1')\n",
    "ax.tick_params(axis='y', labelcolor='#2E86C1')\n",
    "\n",
    "# Plot weight variance on secondary axis\n",
    "train_var = [np.var([weight_history['train'][e][b] for b in branches]) for e in epochs_train]\n",
    "ax2.plot(epochs_train, train_var, marker='^', label='Weight Variance', \n",
    "         color='#27AE60', linewidth=2, linestyle='--')\n",
    "ax2.set_ylabel('Weight Variance', fontsize=11, fontweight='bold', color='#27AE60')\n",
    "ax2.tick_params(axis='y', labelcolor='#27AE60')\n",
    "\n",
    "ax.set_title('Loss & Weight Stability Over Time', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(epochs_train)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/weight_evolution_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ WEIGHT EVOLUTION VISUALIZATION COMPLETE\")\n",
    "print(f\"   - Tracked {len(epochs_train)} training epochs\")\n",
    "print(f\"   - Tracked {len(epochs_val)} validation epochs\")\n",
    "print(f\"   - Final Training Weights: L={weight_history['train'][epochs_train[-1]]['lidar']:.3f}, \"\n",
    "      f\"C={weight_history['train'][epochs_train[-1]]['camera']:.3f}, \"\n",
    "      f\"R={weight_history['train'][epochs_train[-1]]['radar']:.3f}, \"\n",
    "      f\"F={weight_history['train'][epochs_train[-1]]['fused']:.3f}\")\n",
    "print(f\"   - Final Validation Weights: L={weight_history['val'][epochs_val[-1]]['lidar']:.3f}, \"\n",
    "      f\"C={weight_history['val'][epochs_val[-1]]['camera']:.3f}, \"\n",
    "      f\"R={weight_history['val'][epochs_val[-1]]['radar']:.3f}, \"\n",
    "      f\"F={weight_history['val'][epochs_val[-1]]['fused']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== WEATHER-AWARE MODEL: BRANCH PREDICTIONS VS GROUND TRUTH ==========\n",
    "\"\"\"\n",
    "Visualize how the weather-aware trained model performs across all branches.\n",
    "Compare individual branch predictions against ground truth and ensemble output.\n",
    "\"\"\"\n",
    "\n",
    "def visualize_weather_trained_branch_performance(model, loader, num_samples=8, figsize=(18, 10)):\n",
    "    \"\"\"\n",
    "    Create detailed overlaid bar charts showing all 4 branch predictions from weather-aware model.\n",
    "    \n",
    "    Args:\n",
    "        model: trained UTENet4BranchBEV (weather-aware)\n",
    "        loader: DataLoader\n",
    "        num_samples: number of samples to visualize\n",
    "        figsize: figure size\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    sample_data = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if len(sample_data) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            bev_l, bev_c, bev_r, dist = batch\n",
    "            bev_l = torch.nan_to_num(bev_l.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_c = torch.nan_to_num(bev_c.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            bev_r = torch.nan_to_num(bev_r.to(DEVICE), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            dist = torch.nan_to_num(dist.to(DEVICE), nan=50.0, posinf=50.0, neginf=0.0)\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(bev_l, bev_c, bev_r)\n",
    "            pred_ensemble = np.atleast_1d(out[0].cpu().numpy()).flatten()\n",
    "            pred_l = np.atleast_1d(out[1].cpu().numpy()).flatten()\n",
    "            pred_c = np.atleast_1d(out[2].cpu().numpy()).flatten()\n",
    "            pred_r = np.atleast_1d(out[3].cpu().numpy()).flatten()\n",
    "            pred_f = np.atleast_1d(out[4].cpu().numpy()).flatten()\n",
    "            w_l = np.atleast_1d(out[5].cpu().numpy()).flatten()\n",
    "            w_c = np.atleast_1d(out[6].cpu().numpy()).flatten()\n",
    "            w_r = np.atleast_1d(out[7].cpu().numpy()).flatten()\n",
    "            w_f = np.atleast_1d(out[8].cpu().numpy()).flatten()\n",
    "            \n",
    "            dist_gt = np.atleast_1d(dist.cpu().numpy()).flatten()\n",
    "            \n",
    "            # Collect for each sample\n",
    "            for j in range(len(dist_gt)):\n",
    "                sample_data.append({\n",
    "                    'gt': dist_gt[j],\n",
    "                    'ensemble': pred_ensemble[j],\n",
    "                    'branches': {\n",
    "                        'LIDAR': pred_l[j],\n",
    "                        'CAMERA': pred_c[j],\n",
    "                        'RADAR': pred_r[j],\n",
    "                        'FUSED': pred_f[j]\n",
    "                    },\n",
    "                    'weights': {\n",
    "                        'LIDAR': w_l[j] if j < len(w_l) else w_l[0],\n",
    "                        'CAMERA': w_c[j] if j < len(w_c) else w_c[0],\n",
    "                        'RADAR': w_r[j] if j < len(w_r) else w_r[0],\n",
    "                        'FUSED': w_f[j] if j < len(w_f) else w_f[0]\n",
    "                    }\n",
    "                })\n",
    "    \n",
    "    # Create subplots\n",
    "    nrows = (num_samples + 3) // 4\n",
    "    fig, axes = plt.subplots(nrows, 4, figsize=figsize)\n",
    "    if nrows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors_branch = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "    \n",
    "    # Plot each sample\n",
    "    for idx, data in enumerate(sample_data[:num_samples]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        branch_names = list(data['branches'].keys())\n",
    "        branch_preds = list(data['branches'].values())\n",
    "        branch_weights = list(data['weights'].values())\n",
    "        \n",
    "        # Create stacked visualization: bars show predictions\n",
    "        x = np.arange(len(branch_names))\n",
    "        bars = ax.bar(x, branch_preds, color=colors_branch, alpha=0.75, \n",
    "                     edgecolor='black', linewidth=2.5)\n",
    "        \n",
    "        # Add ground truth line\n",
    "        ax.axhline(y=data['gt'], color='red', linestyle='--', linewidth=3, \n",
    "                  label=f\"GT: {data['gt']:.2f}m\", zorder=10)\n",
    "        \n",
    "        # Add ensemble prediction marker\n",
    "        ax.axhline(y=data['ensemble'], color='green', linestyle=':', linewidth=2.5, \n",
    "                  label=f\"Ensemble: {data['ensemble']:.2f}m\", zorder=9)\n",
    "        \n",
    "        # Annotate each bar with prediction and weight\n",
    "        for i, (bar, pred, weight) in enumerate(zip(bars, branch_preds, branch_weights)):\n",
    "            height = bar.get_height()\n",
    "            error = abs(pred - data['gt'])\n",
    "            \n",
    "            # Main label: prediction value\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height/2,\n",
    "                   f\"{pred:.2f}m\",\n",
    "                   ha='center', va='center', fontsize=11, fontweight='bold', \n",
    "                   color='white', bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "            \n",
    "            # Top label: weight and error\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, height + 1.5,\n",
    "                   f\"w:{weight:.2f}\\nerr:{error:.2f}m\",\n",
    "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Styling\n",
    "        max_y = max(max(branch_preds), data['gt']) * 1.25\n",
    "        ax.set_ylim(0, max_y)\n",
    "        ax.set_ylabel('Distance (m)', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'Sample {idx+1}', fontsize=12, fontweight='bold', pad=8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(branch_names, fontsize=9, rotation=0)\n",
    "        ax.legend(fontsize=9, loc='upper right')\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(sample_data), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Weather-Aware Model: Branch Predictions vs Ground Truth',\n",
    "                fontsize=15, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/kaggle/working/weather_aware_branch_predictions.png', dpi=150, bbox_inches='tight')\n",
    "    return fig, sample_data\n",
    "\n",
    "\n",
    "# Generate the weather-aware branch visualization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üå¶Ô∏è  WEATHER-AWARE MODEL: BRANCH PREDICTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "fig_weather, weather_data = visualize_weather_trained_branch_performance(model, test_loader, num_samples=8)\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"\\nüìä DETAILED PERFORMANCE BREAKDOWN:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "branch_errors = {'LIDAR': [], 'CAMERA': [], 'RADAR': [], 'FUSED': [], 'ENSEMBLE': []}\n",
    "\n",
    "for idx, data in enumerate(weather_data[:8]):\n",
    "    print(f\"\\nüîç SAMPLE {idx+1}:\")\n",
    "    print(f\"   Ground Truth: {data['gt']:.2f}m\")\n",
    "    \n",
    "    ensemble_error = abs(data['ensemble'] - data['gt'])\n",
    "    branch_errors['ENSEMBLE'].append(ensemble_error)\n",
    "    print(f\"   Ensemble:    {data['ensemble']:.2f}m (Error: {ensemble_error:.3f}m)\")\n",
    "    \n",
    "    print(f\"\\n   Branch Performance:\")\n",
    "    \n",
    "    for branch in ['LIDAR', 'CAMERA', 'RADAR', 'FUSED']:\n",
    "        pred = data['branches'][branch]\n",
    "        weight = data['weights'][branch]\n",
    "        error = abs(pred - data['gt'])\n",
    "        error_pct = (error / data['gt'] * 100) if data['gt'] > 0 else 0\n",
    "        \n",
    "        branch_errors[branch].append(error)\n",
    "        \n",
    "        # Compare to ensemble\n",
    "        if error < ensemble_error:\n",
    "            indicator = \"‚úÖ BETTER than ensemble\"\n",
    "        elif error > ensemble_error * 1.1:  # Allow 10% tolerance\n",
    "            indicator = \"‚ùå WORSE than ensemble\"\n",
    "        else:\n",
    "            indicator = \"‚ûñ SIMILAR to ensemble\"\n",
    "        \n",
    "        print(f\"      {branch:8} ‚Üí {pred:.2f}m  (w:{weight:.3f}, err:{error:.3f}m ¬±{error_pct:.1f}%) {indicator}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà AGGREGATE STATISTICS (Across All Samples):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for branch, errors in branch_errors.items():\n",
    "    mean_err = np.mean(errors)\n",
    "    std_err = np.std(errors)\n",
    "    rmse = np.sqrt(np.mean(np.array(errors)**2))\n",
    "    \n",
    "    if branch == 'ENSEMBLE':\n",
    "        print(f\"\\nüéØ {branch:10} ‚Üí Mean Error: {mean_err:.3f}m ¬± {std_err:.3f}m | RMSE: {rmse:.3f}m\")\n",
    "    else:\n",
    "        print(f\"   {branch:10} ‚Üí Mean Error: {mean_err:.3f}m ¬± {std_err:.3f}m | RMSE: {rmse:.3f}m\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ WEATHER-AWARE MODEL EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8928097,
     "sourceId": 14014638,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
